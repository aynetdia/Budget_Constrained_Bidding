{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aynetdia/Budget_Constrained_Bidding/blob/master/Tutorial.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the notebook in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount on GDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning the repo (only when setting up for the first time)\n",
    "%cd drive/MyDrive\n",
    "!git clone https://github.com/aynetdia/budget_constrained_bidding.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a link to the dataset: https://drive.google.com/drive/folders/1YYyxGMDW0EuZA2BI-uR_2j60lpngKgy8?usp=sharing\n",
    "\n",
    "In order to set up the dataset, you either have to download the linked folder containing the dataset and put it into `/budget_constrained_biddig/data/ipinyou/`, in case you choose to run the notebook locally, or add a GDrive shortcut (go to: Shared with me -> Right click on the dataset folder -> Add shortcut to Drive) and select the  same `/budget_constrained_biddig/data/ipinyou/` as a location to place the shortcut in, if you want to run the notebook in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the project folder and pull the last changes (do that every time before running the notebook)\n",
    "# If executing after cloning the repo: %cd budget_constrained_bidding\n",
    "%cd /content/drive/MyDrive/budget_constrained_bidding\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Introduction into the application domain\n",
    "2. Methods\n",
    "3. DRLB Framework\n",
    "4. Description of the Data Set\n",
    "5. Implementation\n",
    "6. Results \n",
    "7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocating budget efficiently is a problem that has received a lot of attention in marketing literature.\n",
    "Budget constrained bidding, as an automated bidding strategy and a common scenario in Real-Time Bidding (RTB), is an important concept in online display advertising. In the context of online marketing strategies, budget constrained bidding serves as an important mechanism in which a typical optimization goal for advertisers is to maximize the total value of winning impressions under a certain budget constraint. A winning impression value is usually associated with the expected outcome of an advertisement, e.g. a click or conversion as user feedback. In RTB advertisers give a bid for every individual impression of their advertisements instead of settling for a predefined fixed bid for each campaign. RTB enables advertisers to spend campaigns budget on high-quality ad impressions in order to obtain that positive user feedback. Thus, one challenge is to capture those valuable ad impressions in real-time, which can be solved by an appropriate bidding strategy.   \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budget Constraint Bidding\n",
    "\n",
    "A bidding strategy is based on the evaluation of impressions on an ad campaign, e.g. by the predicted click-through rate (CTR) or calculated bid price for each impression. A campaign runs during a certain time period that captures $N$* impression opportunities in sequential order $i$*. Considering the competition with other bidders in real-time and the impression value $v{_i}$*, an advertiser gives a bid $b{_i}$* and only wins the auction if $b{_i}$* is the highest bid [Wu D., et al., 2018]. In this case the advertiser gets the opportunity to display the ad and to gain winning impressions which then induces costs $c{_i}$*.\n",
    "However, budget constrained bidding is a knapsack problem [Zhou Y., et al., 2008]. An optimal bid in RTB with budget constraint B takes the from of $ b{_i} = v / λ $* , where $v$* denotes the impression value and $λ$* a scaling parameter mapping v to a scale of bid. If $λ$ increases, then the bid price decreases. Hence, high constrained budget should cause the general bidding price level to be lower [Zhang W., et al., 2014].\n",
    "Bidding strategies usually consider the bid decision by treating each ad impression independently, ignoring the impacts of the overall effectiveness of the campaign and the dynamics of the RTB environment. Therefore, developing an optimal bidding strategy is complex to derive but yet highly important since in RTB only the advertiser with the highest bid wins the auction. Reinforcement Learning (RL) is a solution for the above problem.  \n",
    "\n",
    "\n",
    "This tutorial presents a solution for the budget constrained bidding problem by leveraging model-free reinforcement learning, where state spaces are the impressions feature parameter as well as the auction information and the action is to set a bidding price. The tutorial is based on the findings of Wu D., et al. [2018] presented in „Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising“. \n",
    "In this tutorial, we first introduce basic concepts of Reinforcement Learning, Constrained Markov Decision Process and Deep Q-Networks. In the next section we explain the Deep Reinforcement Learning to Bid (DRLB) framework, followed by a brief description of the dataset and an explanation on the implementation of the algorithms. Lastly, we discuss the results. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Introduction to Reinforcement Learning and methods comparison\n",
    "\n",
    "*Reinforcement Learning*  \n",
    "Reinforcement Learning (RL) is a tool to support sequential decision-making events and is a highly active research field of machine learning. It finds enormous application in e.g. real-time-bidding in advertisement auction.\n",
    "\n",
    "RL is a core control problem in which an agent sequentially interacts with an unknown environment to maximize its cumulative reward [1]. It is a simulation of the human learning process in dynamic environments without any supervision. To find the best strategy of reward maximization, the agent continuously interacts with the environment and finds the optimal action under different states. Roughly speaking, the agent’s goal is to get as much reward as possible over the long run.\n",
    "\n",
    "For instance, board game playing is a classical dynamic decision-making process. Players need to interact with their counterparts. It is a procedure filled with immediate rewards, intuitive judgments, and each action will generate an impact in the end. The cumulative reward will decide its win-or-failure [1].\n",
    "\n",
    "In reality, such scenarios will consist of several problems that can be solved by a class of solution methods, but not all of them can achieve the rewards maximization objective. In the learning process of RL, each action the agent made, will generate some impact not only on the immediate reward but also on the future states. In other words, RL faces a dynamic decision-making problem, the agent will obtain continuous rewards and punishments in this learning process and modify its behaviors according to the environment's feedbacks and finally to maximize the cumulative reward.\n",
    "\n",
    "In order to make the process more clear, we explain some basic concept of RL as follows: \n",
    "Policy: the whole actions the learning agent has taken in a concrete period \n",
    "Agent: the one who takes the action\n",
    "Environment: the place where the agent takes action and interacts with him\n",
    "Action: the move the agent makes to interact with the environment\n",
    "State: a situation in which the agent perceives \n",
    "Reward: feedback of the agent’s action \t\n",
    "\n",
    " Image 1 illustrates the whole process of RL.\n",
    "\n",
    "![Image 1: Reinforcement Learning Process](https://drive.google.com/uc?export=view&id=1y_z3Xg_QCsK9kkfV3tWhtqJNDmW4PrAJ)\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "*Methods comparison*  \t\n",
    "Besides RL, supervised learning and unsupervised learning are the most classic machine learning methods. In this section, we explain why RL is the best choice for solving dynamic decision-making problems. Thus, the core differences among these three methods are shown in table(#).\n",
    "\n",
    "\n",
    "![Table 1: Machine Learning Methods Comparison](https://drive.google.com/uc?export=view&id=1Af_FgYtCeLyFwuKVm5UV9JrZpQhK1D2w)\n",
    "\n",
    "\n",
    "Supervised learning is mainly used to solve regression and classification problems. It explores the relation among labeled, the target variable, and other input variables and produces a model to predict further category identification.\n",
    "\n",
    "In comparison, unsupervised learning obtains unlabeled observations and searches the hidden structure behind the input data. Both of these methods are not suitable for the desired behavior optimization problem in a dynamic process. Unlike supervised and unsupervised learning, which will not consider the capacity or some constrained environments, reinforcement learning will start with a clear initial setting, goal, and interactive agent. All the sequential action the agent takes will generate some influence in this environment. After a dynamic model training process, the agent obtains more and more rewards as desired by taking optimal actions in order to maximize the cumulative reward. \n",
    "\n",
    "Reinforcement Learning models problems into a Markov Decision Process and matching the output to the problem. In theory, when we can map our problems into an MDP, we can expect that reinforcement learning can be a useful tool to solve such problems. In the next section, we will explain how it works with Markov Decision Process and give more details about Deep Q-Learning, which is a core concept in our project.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 (Constrained) Markov Decision Process  \n",
    "  \n",
    "*Markov Decision Process*   \n",
    "RL is usually modeled as a Markov Decision Process (MDP). MDP determines how good the agents particular state $s$ is. This is defined by a policy $\\pi: S —> P(A)$, i.e. what actions $A$ the agent performs in a particular state $s$ [Singh A., 2019]. The policy is a function that defines the probability distribution over actions $(a∈A)$ for each state $(s∈S)$ which fully determines the behavior of an agent. The agent uses the policy to interact with the environment by sequentially taking actions and to alter its behavior in order to maximize the cumulative reward $R$ (all the rewards that the agent receives from the environment) [Wu D., et al., 2018]. In MDP the state transition $T(s, a, s’)$ produces the probability $Pr(s’ | s, a)$ to transition into state $s’$ given that the agent was in state $s$ and took action $a$. The immediate reward function, which is the reward $r$ an agent receives from the current state, shows the agents immediate reward from the environment. It will make the agent converge to suboptimal solutions, since the agent greeds to take actions to decrease $λ$ in order to gain more immediate reward each time [Wu D., et al., 2018]. If an agent follows a policy $\\pi$  at time $t$  then $\\pi(a | s)$ is the probability of that agent taking action $a$ at a particular time step $t$ [Singh A., 2019].\n",
    "The goal of the agent is to learn the optimal policy \\pi * which maximizes the long-term expected reward form the start state [Wu D., et al., 2018]: $max \\pi = arg max_{\\pi}  E [R | \\pi]$ , where $R = \\sum^T_{t=1} \\gamma^{t-1} r_t $ denotes the cumulative discounted reward.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation for value function\n",
    "\n",
    "The Bellman Equation helps to determine optimal policies and optimal value functions, where value functions differ according to the different policies.\n",
    "The value function expresses the relationship between the value of a state $s$ and the values of its successor states $s’$ . The value of a state under an optimal policy must equal the expected reward for the best action from that state. It is decomposed into the Immediate Reward, $R[t+1]$ , and the discounted value of successor states [Singh A., 2019]:\n",
    "$v(s)=E[R_{t+1}+\\gamma+v(S_{t+1}) | S{_t} = s]$ .\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Markov Decision Process\n",
    "\n",
    "Constrained Markov Decision Process (CMDP) extends the concept of MDP by adding additional cost constraints, i.e. the total budget [Wu D., et al., 2018 ; Altman E., 1999]. In CMDP the action of the agent is to submit bids to sequential impression opportunities. In an episodic CMDP the agent regulates $λ$ with a fixed time step T until the episode ends. The agent observes states and takes action to adjust $λ_{t-1}$ to $λ_{t}$ during each time step $t$. However, the agent will still converge to the suboptimal solution due to the immediate reward function.\n",
    "Considering the dynamic and unpredictable RTB environment, makes it hard to obtain the transition dynamic. This is due to the fact that CMDP is usually a model-based RL approach, where the state transition dynamics $T$ needs to be known in advance. Therefore, our focal paper proposes to use deep Q-Networks (DQN) which is a model-free RL approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Deep Q-Networks\n",
    "\n",
    "\n",
    "DQN is employed to model budget constrained bidding. It is a popular method of Q-learning in RL. DQN iteratively learns the action-value function $Q (s_{t}, a_{t})$ corresponding to the optimal policy. It quantifies the quality of taking action $a_{t}$ at state $s_{t}$ [Wu D., et al., 2018]. The agent uses deep neural networks to determine its actions and to train the neural network in such a way that it predicts the weighted, cumulative rewards of all actions. In DQN the agent learns the optimal policy by interacting directly with the workflow environment and also with the policies of other agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. DRLB framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Description of the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you should find yourself inside the project folder after you've pulled the latest changes\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "bid_requests = pd.read_csv('data/ipinyou/1458/train.log.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add the necessary data intervals\n",
    "\n",
    "def get_time_interval(data):\n",
    "    time_inv=int(data[10:12])\n",
    "    if time_inv>=0 and time_inv<15:\n",
    "            return (\"00\")\n",
    "    elif time_inv >= 15 and time_inv < 30:\n",
    "            return (\"15\")\n",
    "    elif time_inv >= 30 and time_inv < 45:\n",
    "            return (\"30\")\n",
    "    elif time_inv >= 45 and time_inv <=60:\n",
    "            return (\"45\")\n",
    "    else:\n",
    "            return(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'min' in bid_requests.columns:\n",
    "    print(\"min already exists\")\n",
    "    pass\n",
    "else:\n",
    "    bid_requests[\"timestamp\"]=bid_requests[\"timestamp\"].apply(str)\n",
    "    min_intervals = bid_requests.apply(lambda row : get_time_interval(row['timestamp']), axis = 1)\n",
    "    bid_requests.insert(3, \"min\", min_intervals) # insert the new column after the 'hour' column\n",
    "    # save the updated dataset. do not run again!\n",
    "    bid_requests.to_csv('data/ipinyou/1458/train.log.txt', sep=\"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#if error \"No module named 'lru'\" pop up, uncomment next line\n",
    "#!pip install  lru-dict\n",
    "\n",
    "# Train DRLB\n",
    "\n",
    "import sys  \n",
    "import torch\n",
    "import cloudpickle\n",
    "sys.path.insert(0, 'src/rtb_agent')\n",
    "from rl_bid_agent import RlBidAgent\n",
    "import gym, gym_auction_emulator\n",
    "\n",
    "env = gym.make('AuctionEmulator-v0')\n",
    "env.seed(0)\n",
    "agent = RlBidAgent()\n",
    "\n",
    "obs, done = env.reset()\n",
    "train_budget = env.bid_requests.payprice.sum()/8\n",
    "\n",
    "budget_proportions = []\n",
    "for episode in env.bid_requests.weekday.unique():\n",
    "    budget_proportions.append(len(env.bid_requests[env.bid_requests.weekday == episode])/env.total_bids)\n",
    "for i in range(len(budget_proportions)):\n",
    "    budget_proportions[i] = round(train_budget * budget_proportions[i])\n",
    "agent.episode_budgets = budget_proportions\n",
    "agent._reset_episode()\n",
    "agent.cur_day = obs['weekday']\n",
    "agent.cur_hour = obs['hour']\n",
    "agent.cur_state = agent._get_state() # observe state s_0\n",
    "\n",
    "epochs = 400\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    while not done:\n",
    "        bid = agent.act(obs) # obs = state\n",
    "        next_obs, cur_reward, cur_cost, win, done = env.step(bid)\n",
    "        agent._update_reward_cost(bid, cur_reward, cur_cost, win)\n",
    "        obs = next_obs\n",
    "    print(\"Total Impressions won {} value = {}\".format(agent.total_wins, agent.total_rewards))\n",
    "    if ((epoch + 1) % 20) == 0:\n",
    "        PATH = 'models/model_state_{}.tar'.format(epoch)\n",
    "        torch.save({'local_q_model': agent.dqn_agent.qnetwork_local.state_dict(),\n",
    "                    'target_q_model':agent.dqn_agent.qnetwork_target.state_dict(),\n",
    "                    'q_optimizer':agent.dqn_agent.optimizer.state_dict(),\n",
    "                    'rnet': agent.reward_net.reward_net.state_dict(),\n",
    "                    'rnet_optimizer': agent.reward_net.optimizer.state_dict()}, PATH)\n",
    "\n",
    "        f = open('models/rnet_memory_{}.txt'.format(epoch), \"wb\")\n",
    "        cloudpickle.dump(agent.dqn_agent.memory, f)\n",
    "        f.close()\n",
    "        f = open('models/rdqn_memory_{}.txt'.format(epoch), \"wb\")\n",
    "        cloudpickle.dump(agent.reward_net.memory, f)\n",
    "        f.close()\n",
    "\n",
    "        pd.DataFrame(agent.step_memory).to_csv('models/step_history_{}.csv'.format(epoch),header=None,index=False)\n",
    "        agent.step_memory=[]\n",
    "        pd.DataFrame(agent.episode_memory).to_csv('models/episode_history_{}.csv'.format(epoch),header=None,index=False)\n",
    "        agent.step_memory=[]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#random bidding\n",
    "class Random_Bidding():\n",
    "\n",
    "\n",
    "        def random_bidding(self,highest_bid):\n",
    "\n",
    "                #generate a random number generator (0,300)\n",
    "                #get a dataframe with columns('click', 'slotprice', 'payprice','r_bid_price','wins')\n",
    "                c_names=('click','day', 'slotprice', 'payprice','r_bid_price','wins')\n",
    "                zero_Data=np.zeros(shape=(self.total_bids,len(c_names)))\n",
    "                self.df=pd.DataFrame(zero_Data,columns=c_names)\n",
    "                self.df['day']=self.bid_requests['weekday']\n",
    "                self.df['click']=self.bid_requests['click']\n",
    "                self.df['slotprice'] = self.bid_requests['slotprice']\n",
    "                self.df['payprice'] = self.bid_requests['payprice']\n",
    "                self.df['r_bid_price']=np.random.randint(0,highest_bid,[self.total_bids,1])\n",
    "\n",
    "\n",
    "                self.rem_budget=self.budget\n",
    "                self.cur_day=str(int(self.df['day'][0]))\n",
    "                #print(range(len(self.df)))\n",
    "                for i in range(len(self.df)):\n",
    "                        if str(int(self.df.loc[i]['day'])) != self.cur_day:\n",
    "                                self.cur_day=str(int(self.df.loc[i]['day']))\n",
    "                                self.rem_budget=self.budget\n",
    "\n",
    "                        cost=self.df.loc[i]['r_bid_price']\n",
    "                        if self.rem_budget<cost:\n",
    "                                self.df.loc[i]['r_bid_price']=self.rem_budget\n",
    "                                cost = self.df.loc[i]['r_bid_price']\n",
    "\n",
    "                        if self.rem_budget<=0:\n",
    "                                  self.df.loc[i]['r_bid_price']=0\n",
    "                                  cost=0\n",
    "\n",
    "                        self.rem_budget-=(cost/1e9)\n",
    "\n",
    "\n",
    "\n",
    "                def wins_value(row):\n",
    "                        if row['r_bid_price'] >= row['slotprice'] and row['r_bid_price'] > row['payprice']:\n",
    "                                return 1\n",
    "                        return 0\n",
    "\n",
    "                self.df['wins'] = self.df.apply(wins_value, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rb=Random_Bidding()\n",
    "rb.budget = 1000\n",
    "rb.bid_requests=bid_requests\n",
    "rb.total_bids=len(bid_requests)\n",
    "rb.random_bidding(highest_bid=100)\n",
    "\n",
    "click1=sum(rb.df['click'])\n",
    "wins1=rb.total_bids\n",
    "wins2=sum(rb.df['wins'])\n",
    "click2=sum(rb.df.loc[rb.df['wins']==1]['click'])\n",
    "\n",
    "\n",
    "print(\"Total actual random winning Impressions = {} clicks = {} \\n;\".format(wins1,click1),\n",
    "      \"Total random winning Impressions = {} clicks = {}\".format(wins2, click2))\n",
    "\n",
    "#rb.df.to_csv(os.getcwd() + '\\data\\rd_bid.txt', header=True,  sep=' ', mode='a')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. References \n",
    "\n",
    "Altman E. (1999): „Constrained Markov decision processes“, CRC Press, Vol. 7.  \n",
    "Singh A. (2019): „Reinforcement Learning : Markov-Decision Process (Part 1)“, [online] https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da [08.12.2020].  \n",
    "Wu D., Chen X., Yang X., et al. (2018): „Budget Constrained Bidding by model-free Reinforcement Learning in Display Advertising“, Association for Computing Machinery, 27, pp. 1443–1451.  \n",
    "Zhang W., Yuan S., and Wang J. (2014): „Optimal Real-Time Bidding for Display Advertising“ Association for Computing Machinery, 20, pp. 1077–1086.  \n",
    "Zhou Y., Chakrabarty D., and Lukose R. (2008): „Budget constrained bidding in keyword auctions and online knapsack problems“, International Workshop on Internet and Network Economics, Springer, pp. 566–576.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb=Random_Bidding()\n",
    "rb.budget = 1000\n",
    "rb.bid_requests=bid_requests\n",
    "rb.total_bids=len(bid_requests)\n",
    "rb.random_bidding(highest_bid=100)\n",
    "\n",
    "click1=sum(rb.df['click'])\n",
    "wins1=rb.total_bids\n",
    "wins2=sum(rb.df['wins'])\n",
    "click2=sum(rb.df.loc[rb.df['wins']==1]['click'])\n",
    "\n",
    "\n",
    "print(\"Total actual random winning Impressions = {} clicks = {} \\n;\".format(wins1,click1),\n",
    "      \"Total random winning Impressions = {} clicks = {}\".format(wins2, click2))\n",
    "\n",
    "#rb.df.to_csv(os.getcwd() + '\\data\\rd_bid.txt', header=True,  sep=' ', mode='a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. References \n",
    "\n",
    "Altman E. (1999): „Constrained Markov decision processes“, CRC Press, Vol. 7.  \n",
    "Singh A. (2019): „Reinforcement Learning : Markov-Decision Process (Part 1)“, [online] https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da [08.12.2020].  \n",
    "Wu D., Chen X., Yang X., et al. (2018): „Budget Constrained Bidding by model-free Reinforcement Learning in Display Advertising“, Association for Computing Machinery, 27, pp. 1443–1451.  \n",
    "Zhang W., Yuan S., and Wang J. (2014): „Optimal Real-Time Bidding for Display Advertising“ Association for Computing Machinery, 20, pp. 1077–1086.  \n",
    "Zhou Y., Chakrabarty D., and Lukose R. (2008): „Budget constrained bidding in keyword auctions and online knapsack problems“, International Workshop on Internet and Network Economics, Springer, pp. 566–576.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}