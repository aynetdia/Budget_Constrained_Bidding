{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aynetdia/Budget_Constrained_Bidding/blob/master/Tutorial.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the notebook in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount on GDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning the repo (only when setting up for the first time)\n",
    "%cd drive/MyDrive\n",
    "!git clone https://github.com/aynetdia/budget_constrained_bidding.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a link to the dataset: https://drive.google.com/drive/folders/1YYyxGMDW0EuZA2BI-uR_2j60lpngKgy8?usp=sharing\n",
    "\n",
    "In order to set up the dataset, you either have to download the linked folder containing the dataset and put it into `/budget_constrained_biddig/data/ipinyou/`, in case you choose to run the notebook locally, or add a GDrive shortcut (go to: Shared with me -> Right click on the dataset folder -> Add shortcut to Drive) and select the  same `/budget_constrained_biddig/data/ipinyou/` as a location to place the shortcut in, if you want to run the notebook in Colab.\n",
    "\n",
    "Link to the Github repo: https://github.com/aynetdia/budget_constrained_bidding.git\n",
    "\n",
    "If needed, all the package requirements are listed in the requirements_pip.txt (for pip envs) and requirements_conda.txt (for conda envs) files, contained inside the Github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the project folder and pull the last changes (do that every time before running the notebook)\n",
    "# If executing after cloning the repo: %cd budget_constrained_bidding\n",
    "%cd /content/drive/MyDrive/budget_constrained_bidding\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "&emsp;&emsp;1. Introduction into the application domain\n",
    "\n",
    "&emsp;&emsp;2. Methods\n",
    "\n",
    "&emsp;&emsp;3. DRLB Framework\n",
    "\n",
    "&emsp;&emsp;4. Description of the Data Set\n",
    "\n",
    "&emsp;&emsp;5. DRLB Implementation\n",
    "\n",
    "&emsp;&emsp;6. DRLB Results \n",
    "\n",
    "&emsp;&emsp;7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocating budget efficiently is a problem that has received a lot of attention in marketing literature. Budget constrained bidding, as an automated bidding strategy and a common scenario in Real-Time Bidding (RTB), is an important concept in online display advertising. This tutorial addresses the problem of budget allocation in marketing context and presents a solution for budget constrained bidding problem, which leverages model-free reinforcement learning. The tutorial is based on the findings of Wu D., et al. (2018) presented in \"Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising\". It proposes a model-free deep reinforcement learning method to solve the budget constrained bidding problem in RBT display advertising.\n",
    "\n",
    "In this tutorial, we first explain how real-time bidding works and what budget constrained bidding problem is. Then we introduce some basic concepts of Reinforcement Learning, Constrained Markov Decision Process and Deep Q-Networks. In the next section we explain the Deep Reinforcement Learning to Bid framework, followed by a brief description of the dataset and an explanation on the implementation of the algorithms. Lastly, we discuss the results.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Real-time bidding   \n",
    "\n",
    "\n",
    "Real-time bidding is a trading mechanism for online display advertising. It is an auction and describes the mechanism of advertisers setting a bid price for every ad impression (i.e. the ad placement) in real-time (Liu et al. , 2019).  \n",
    "\n",
    "Figure 1 illustrates the following process of RTB. An ad impression is created when a user visits an ad-supported site (e.g. webpage or mobile apps) which then triggers an auction. Then an ad requests is sent by the supply-side platform (SSP, such as a webpage) to the ad exchange (ADX, that publishes the bid requests to the DSPs and that also holds auctions to determine which campaign win the ad impression). Accordingly, the demand-side platform (DSP, i.e. the advertisers’ buying systems) generates different bid prices. A bid price is based on users features (e.g. region) and ad features (e.g. ad dimension) extracted from the data management platform (DMP) or the webpage itself. The ADX then determines the winning advertiser (Liu et al. , 2019). Solely the highest bidder wins the ad impression opportunity to display its ad on the webpage.   \n",
    "\n",
    "According to the widely used second-price auction principal, the winning advertiser pays the second highest bid in the auction (Wu et al., 2018). This cost basically is defined as the market price (Zhang et al. , 2014). The second-price auction ensures that the bid price for each ad impression should be equal to its true value of an action (e.g. click-through-rate). \n",
    "A challenge is to capture those valuable ad impressions in real-time, which can be solved by an appropriate bidding strategy.\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=14dbywvUp94LlXv3sjL1VpDCs10SuYLRN\" />\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "Fig. 1: RTB \n",
    "<p>  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Budget Constrained Bidding\n",
    "\n",
    "Budget Constrained Bidding is an automated bidding strategy in RTB. It serves as a mechanism to address the typical optimization goal, where advertisers opt to maximize the total value of winning impressions under a certain budget constraint. Generally, an impression is the presence of an online advertisement on e.g. a browsing website. A winning impression value is associated with the expected outcome of an advertisement, e.g. a click on the ad or a conversion as user feedback.  \n",
    "\n",
    "A bidding strategy is based on the evaluation of impressions on an ad campaign, e.g. by the predicted click-through rate (CTR) or calculated bid price for each impression.   \n",
    "A campaign runs during a certain time period that captures $N$ impression opportunities in sequential order $i$. Considering the competition with other bidders in real-time and the impression value $v{_i}$ of an ad placement, an advertiser gives a bid $b{_i}$ and only wins the auction if $b{_i}$ is the highest bid (Wu D., et al., 2018). In this case the advertiser gets the opportunity to display the ad and and thus, wins the ad impression, which then induces costs $c{_i}$. An optimal bid in RTB with budget constraint B takes the from of:\n",
    "\n",
    "$$ b{_i} = v{_i} / \\lambda, $$\n",
    "\n",
    "where $v{_i}$ denotes the impression value of an ad placement and $\\lambda$ a scaling parameter mapping $v{_i}$ to the scale of the bid (Zhang W., et al., 2016). If $\\lambda$ increases, then the bid price decreases. Hence, high constrained budget should cause the general bidding price level to be lower (Zhang W., et al., 2014).  \n",
    "\n",
    "However, the optimal bidding strategy depends on the market competition, auction volume and campaign budget. Bidding strategies usually consider the bid decision by treating each ad impression independently, ignoring the impacts of the overall effectiveness of the campaign and the dynamics of the RTB environment. Since the market price (second highest bid price) is unknown during the budget constrained bidding in real-time bidding environment, developing an optimal bidding strategy is complex to derive but yet crucial for advertisers to maximize their budget cost efficiency of each ad campaign. Reinforcement Learning (RL) is a solution for the above problem.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Knapsack problem\n",
    "\t\n",
    "The basic idea of the Knapsack problem is to find an optimal strategy under some constraints. For example, how to choose the most suitable items to be placed in the backpack in order to maximize the values of the items, given a limited total weight. Such problems can usually be solved recursively, i.e., by converting the original problem into an iterable subproblem, and then solving it optimally by dynamic programming methods. KP is decomposed into multiple P, each instance P’ in RL represents a P and will be described as [ state,action,reward ] (Reza R.A, Yingqian Z., Murat F. et al， 2020). Regarding our research subject, budget constrained bidding is basically a knapsack problem (Zhou Y., et al., 2008). In this paper, we will apply Reinforcement Learning to efficiently allocate given budget and try to gain maximum rewards.\n",
    "  \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1iUdccP4aFYMTZ8y9oSgO57-1G3mvyNR_\" width=300 height=200 />\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Introduction to Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "Reinforcement Learning (RL) is a tool to support sequential decision-making events and is a highly active research field of machine learning. It finds enormous application in e.g. real-time-bidding in advertisement auction.\n",
    "\n",
    "In order to make the process more clear, we first explain some basic concept of RL as follows:\n",
    "\n",
    "- Policy: the whole actions the learning agent has taken in a concrete period \n",
    "\n",
    "- Agent: the one who takes the action\n",
    "\n",
    "- Environment: the place where the agent takes action and interacts with him\n",
    "\n",
    "- Action: the move the agent makes to interact with the environment\n",
    "\n",
    "- State: a situation in which the agent perceives \n",
    "\n",
    "- Reward: feedback of the agent’s action \t\n",
    "\n",
    "\n",
    "RL is a solution for core control problems in which an agent sequentially interacts with an unknown environment to maximize its cumulative reward (Richard S. and Andrew G., 2018). It is a simulation of the human learning process in dynamic environments without any supervision. To find the best strategy of reward maximization, the agent continuously interacts with the environment and finds the optimal action under different states. Roughly speaking, the agent’s goal is to get as much reward as possible over the long run.\n",
    "\n",
    "For instance, board game playing is a classical dynamic decision-making process. Players need to interact with their counterparts. It is a procedure filled with immediate rewards, intuitive judgments, and each action will generate an impact in the end. The cumulative reward will decide its win-or-failure (Richard S. and Andrew G., 2018).\n",
    "\n",
    "In reality, such scenarios will consist of several problems that can be solved by a class of solution methods, but not all of them can achieve the rewards maximization objective. In the learning process of RL, each action the agent made, will generate some impact not only on the immediate reward but also on the future states. In other words, RL faces a dynamic decision-making problem, the agent will obtain continuous rewards and punishments in this learning process and modify its behaviors according to the environment's feedbacks and finally to maximize the cumulative reward.\n",
    "\n",
    "\n",
    "And the following image illustrates how typically agents interact with their environments in RL-based models:\n",
    "![Image 1: Reinforcement Learning Process](https://drive.google.com/uc?export=view&id=1y_z3Xg_QCsK9kkfV3tWhtqJNDmW4PrAJ) \n",
    " <!--<img src=\"img/process_RL.png\" width=600 height=600 /> -->\n",
    "\n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Markov Decision Process\n",
    "\n",
    "The Markov Decision Process (MDP) is a popular mechanisms to model Reinforcement Learning problems. MDP (Sutton and Barto, 2014) provides a framework that is widely used for modeling the dynamic agent-environment interactions - as mentioned and depicted in figure 2.   \n",
    "\n",
    "An MDP can be represented by a tuple (S, A, P, R), where S and A denotes sets of all states and all possible actions $a$ $\\in$ $A$  in state $s$ $\\in$ $S$, P represents the state transition probability from state $s$ $\\in$ $S$ to another state $s^\\prime$ $\\in$ $S$ when taking action $a$ $\\in$ $A$ that is denoted as $𝑇(𝑠,𝑎,𝑠^\\prime)$ and R represents the reward function that is denoted as $r(a,s,s^\\prime)$. Accordingly, in MDP the interaction process between the agent and its environment is summarized as the following: The agent observes state $s_t$ from the environment and takes action $a_t$ according to the policy strategy $\\pi$ in each time step t∈T (Liu et al., 2019). In MDP the state transition $𝑇(𝑠,𝑎,𝑠^\\prime)$ produces the probability $𝑃(𝑠^\\prime | 𝑠,𝑎)$ to transition from state s into state $𝑠^\\prime$ , given that the agent took action 𝑎. The environment then returns a reward $r_t$ to the agent. The rewards (numerical values) can be positive or negative based on the agents action (Sutton and Barto, 2014,  Liu et al., 2019). \n",
    "The agent then observes the next state and the process repeats. MDP will terminate after T time steps (e.g. after the budget is exhausted) since the time is limited.    \n",
    "\n",
    "\n",
    "In MDP the policy $\\pi$ is a mechanism to take decisions. Policies are defined by the action an agent takes on the current state (it does not depend on history) (Sutton and Barto, 2014). The agent uses the policy (see section 2.3) to interact with the environment by sequentially taking actions and to alter its behavior in order to maximize the cumulative reward 𝑅 (all the rewards that the agent receives from the environment) (Wu D., et al., 2018).  \n",
    "\n",
    "Overall, the goal of the agent is to learn the optimal policy $\\pi^*$ , which maximizes the long-term expected reward form the start state:\n",
    "$$\\pi^* = arg max_{\\pi}  E [R | \\pi],$$ where $R = \\sum^T_{t=1} \\gamma^{t-1} r_t $ denotes the cumulative discounted reward (Wu D., et al., 2018).    \n",
    "  \n",
    "Figure 3 illustrates the overall process of MDP.    \n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=15kJ_shivzm2VBhEMv7U952tHaa9WChfA\" />\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "Fig. 3: Markov decision process (Liu et al., 2019) \n",
    "<p>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Bellman Equation for value functions\n",
    "\n",
    "\n",
    "Reinforcement learning mostly involves estimating **value functions**, which are functions of states (quantifies the value of every state-action pair) (Sutton and Barto, 2014). It is used to estimate how good it is for the agent to be in a given state, i.e. how good (or bad) future rewards are that can be expected from experience (expected return). Since the expected return depends on the action that the agent will take, the value functions are defined respectively to particular policies. \n",
    "As mentioned earlier in section 2.2, MDPs aim to help the agent to find an optimal policy in its environment. Policies are defined by the action an agent takes on the current state. The objective of MDP policies is to maximize the expected return (reward from environment) for the agent. Thus, the policy $\\pi$ of the value function defines the probability distribution of taking action a when in state s (Sutton and Barto, 2014), which essentially determines the behavior of an agent. For instance, this corresponds to the bidding strategy in RTB display advertising. If an agent follows a policy $\\pi$ at time 𝑡 then $\\pi(𝑎|𝑠)$ is the probability of that agent taking action 𝑎 at a particular time step 𝑡. The policy $\\pi$ is defined as follows (Singh A., 2019):    \n",
    "$$\\pi: S \\rightarrow P(A) .$$ \n",
    "So essentially, it is used to estimate how good an agent's actions $a$ $\\in$ $A$ in a particular state $s$ $\\in$ $S$ is.  \n",
    "\n",
    "\n",
    "The **Bellman Equation for value function** $v_\\pi$ expresses the relationship between the value of a state $s$ and the values of its successor states $s^\\prime$ (Sutton and Barto, 2014). It helps to determine optimal policies and optimal value functions. \n",
    "The agent chooses the action according to the imposed policy. We know that policy changes with experience. Hence, different policies implies different value functions. \n",
    "The optimal value function is the one that gives the maximum value. So, we choose a sequence of action that will generate the highest reward which is the cumulative reward (referred to as Quality Value (Q-Value) in section 2.5). We will receive the highest Q-Value from $s^\\prime$  by choosing the action that maximizes the Q-Value.   \n",
    "\n",
    "\n",
    "So, the Bellman Equation is the expectation of reward the agent got on leaving the state $s$ plus the discounted value of the state $s^\\prime$  the agent moved to. The Bellman Equation can be decomposed into the immediate reward, $R[t+1]$ , and the discounted value of successor states:\n",
    "\n",
    "$$v_\\pi(s)=E[R_{t+1} + \\gamma v(S_{t+1}) | S{_t} = s].$$    \n",
    "\n",
    "The discount factor $\\gamma$ controls the importance of longterm rewards versus the immediate rewards. An immediate reward is yielded from being at state s and selecting action a. \n",
    "The value of a state, under an optimal policy, must equal the expected reward for the best action from that state (Sutton and Barto, 2014).     \n",
    "\n",
    "Figure 4 depicts so called backup diagrams, that show the relationship of states which transfer value information back to a state form it’s successor state.   \n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1fLbm9t-NHvMYjA5VJecwLjQizma7vOJV\" />\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "Fig. 4: Backup diagrams for value function (Sutton and Barto, 2014)  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Constrained Markov Decision Process  \n",
    "\n",
    "\n",
    "Constrained Markov Decision Process (CMDP) (Altman E., 1999) extends the concept of MDP by adding additional constraints, e.g. a cost constraint. \n",
    "\n",
    "While learning in an unknown CMDP, an agent should trade-off exploration and exploitation. Accordingly, the agent explores to discover new information about the MDP, and exploits on the current knowledge to maximize the reward while satisfying the constraints.In an episodic CMDP the agent regulates $\\lambda$ with a fixed time step T until the episode ends. The agent observes states and takes action to adjust $\\lambda_{t-1}$ to $\\lambda_{t}$ during each time step $t$. However, the agent will still converge to the suboptimal solution due to the immediate reward function (Wu D., et al., 2018).   \n",
    " \n",
    "Note that the (C)MDP is often solve using model-based reinforcement learning approaches, where the state transition dynamics $T$ needs to be known in advance but is yet hard to predict, when considering the dynamic and unpredictable environment of an agent. Besides, model-based approaches suffer from the scalability problem, since calculations of state transitions matrices, which are then solved using dynamic programming algorithms, require high computational costs, which may be detrimential when deployed in real-world scenarios.\n",
    "\n",
    "In contrast to this, model-free reinforcement learning approaches solves reinforcement learning tasks directly by solely using the policy function. Therefore, our focal paper proposes to use deep Q-Networks (DQN), which is a model-free RL approach. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Deep Q-Networks  \n",
    "\n",
    "\n",
    "Deep Q-Network (DQN) (Mnih, Kavukcuoglu, Silver et al., 2015) combines the methods of reinforcement learning and deep neural networks to map the environment states to the agents actions. DQN is a popular method of Q-learning. \n",
    "\n",
    "**Q-learning** algorithms (Watkins & Dayan, 1992) can be used when the probabilities of transitions are unknown and when the state or action spaces are large. It focus on learning a Q-Function that qualifies a state-action pair. So, the Q-Function tells us the value of performing a certain action or respectively how good it is for the agent to take action (a) in a state (s) with a policy $\\pi$.\n",
    "It iteratively computes the optimal value function, i.e. for each state s at each timestep t , the optimal value Q*(s,a,t) of each action a is estimated (Sutton and Barto, 2014).  \n",
    "\n",
    "A Q-Value (Quality Value) represents the expected longterm reward of a Q-Learning algorithm under the assumption that the agent takes a perfect sequence of actions from a specific state. At any given state, we perform the action that will eventually yield the highest cumulative reward. This makes the algorithm greedy (Mnih, Kavukcuoglu, Silver et al., 2015). \n",
    "\n",
    "Essentially, Q-Learning utilizes the Bellman Equation. To estimate these state/action values, we can then update it using the Bellman Equation as an update rule (Watkins & Dayan, 1992):  \n",
    "$$ Q(s,a) := r(s,a) + \\gamma max_a Q(s^\\prime, a)$$ \n",
    "\n",
    "As mentioned in section 2.3, it roughly states that the maximum future reward for a specific action, is the current reward plus the maximum reward for taking the next action.  \n",
    "\n",
    "\n",
    "However, the main challenge of Q-Leraning and Bellman Equation is to the compute cost associated with estimating all combinations of state-action rewards, involving a large number of states. Thus, we approximate a Q-function instead of learning an exact Q-Function that evaluates all possible Q-Values. This is were DQN comes into play. \n",
    "\n",
    "\n",
    "**DQN** (Mnih, Kavukcuoglu, Silver et al., 2015) uses deep neural networks to approximate the Q-Function corresponding to the optimal policy without the need of handcrafting useful features. Recall that the goal of an agent is to select actions in order to maximize the cumulative reward. This cumulative reward that we receive is often referred to as the Q-Value. Thus, by utilizing deep neural networks we can estimate the Q-Value of all possible actions for a given state.  \n",
    "The Q-Function $Q^*$ is defined as the maximum sum of rewards $r_{t}$ discounted by $\\gamma$ at each tilmestep $t$. This is achieved by the behavior policy $\\pi = P(a|s)$ of the agent. It defines the value of taking action $a$ in state $s$ under policy $\\pi$:\n",
    "\n",
    "$$ Q^*(s,a) = max_\\pi E[r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... | s_t=s , a_t=a, \\pi ].$$  \n",
    "\n",
    "\n",
    "In DQN the parameter $\\epsilon$ of the $\\epsilon$-greedy mechanism controls the proportion of exploration and exploitation, where $\\epsilon$ is bounded at 0 < $\\epsilon$ < 1. The exploitation describes that the agent finds his following action by maximizing the Q-value over all possible actions for the given state.The agent has the change to explore new opportunities.   \n",
    "\n",
    "\n",
    "All in all, the agent uses deep neural networks to determine its actions and to train the neural network in such a way that it predicts the weighted, cumulative rewards of all actions. In DQN the agent learns the optimal policy by interacting directly with the workflow environment and also with the policies of other agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.6 ML paradigm comparison\t\n",
    "\n",
    "\n",
    "\n",
    "Besides RL, supervised learning and unsupervised learning are the most classic machine learning methods. In this section, we explain why RL is the best choice for solving dynamic decision-making problems. Thus, the core differences among these three methods are shown in table below.\n",
    "\n",
    "\n",
    " ![Table 1: Machine Learning Methods Comparison](https://drive.google.com/uc?export=view&id=1U8oQjgdYPhFZ-FMyuyF2MPYSIuXksVkq) \n",
    "<!--<img src=\"img/methods_comparison.png\" width=600 height=600 />-->\n",
    "\n",
    "Supervised learning is mainly used to solve regression and classification problems. It explores the relation among labeled, the target variable, and other input variables and produces a model to predict further category identification.\n",
    "\n",
    "In comparison, unsupervised learning obtains unlabeled observations and searches the hidden structure behind the input data. Both of these methods are not suitable for the desired behavior optimization problem in a dynamic process. Unlike supervised and unsupervised learning, which will not consider the capacity or some constrained environments, reinforcement learning will start with a clear initial setting, goal, and interactive agent. All the sequential action the agent takes will generate some influence in this environment. After a dynamic model training process, the agent obtains more and more rewards as desired by taking optimal actions in order to maximize the cumulative reward. \n",
    "\n",
    "Reinforcement Learning models heavily lean on the concept of a Markov Decision Process, which is used to formulate a given problem and thus find an optimal solution. Generally, when we can map our problems into an MDP, we can expect that reinforcement learning can be a useful tool to solve such problems. In the next section, we will explain the details behind Markov Decision Processes and how they are constructed, and give more details about Deep Reinforcement Learning to Bid, which is the main focus in our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Deep Reinforcement Learning to Bid (DRLB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To address the problem of budget constrained bidding in real-time advertisement, Wu et al. (2018) proposed a framework that is built on top of DQN - Deep Reinforcement Learning to Bid (DRLB). The advantage of this framework for RTB is given by the fact that it is a model-free RL approach.\n",
    "\n",
    "Thus, Wu et al. proposed training a **DQN agent** that sequentially regulates the bidding parameter $\\lambda$ (remember, the bid in DRLB takes form of $b_i = v_i/{\\lambda}$) instead of directly producing bids, which is the case in model-based approaches like the one introduced by Cai et al. (2017). Furthermore, instead of using the immediate reward from the environment to determine an optimal policy, an additional deep neural network (called **RewardNet**) is trained simultaneously with the DQN to predict the reward. Wu et al. (2018) found that simply using the immediate reward will make the agent obsessed with taking actions to agressively decrease $\\lambda$ from the get ho to gain more immediate reward at each step and thus effectively neglect the budget constraint.\n",
    "\n",
    "Additionally an **adaptive $\\epsilon$-greedy policy**, that adjusts the exploration probability based on how the state-action value is distributed, was designed for DRLB. Hence the agent chooses action (adjusting the $\\lambda$ parameter):\n",
    "\n",
    "$$ a^* = argmax_a Q(s,a) $$\n",
    "\n",
    "with probability $1 -\\epsilon$ or otherwise takes a random action. $\\epsilon$ is usually initialized with a large value and then gradually annealed to a small value over time. This balances the exploration-exploitation trade-off during training, by encouraging the agent to take more random actions in the beginning of the training process (and thus explore a variety of actions), and rely on the learned DQN parameters (exploit them) further down the training process. \n",
    "\n",
    "Before delving more into details of DRLB implementation, we'll describe the dataset that was used to train and evaluate the DRLB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Description of the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This section gives a brief insight into the data structure and depicts the results of some explanatory data analysis (EDA) to provide a better understanding of the data.    \n",
    "\n",
    "The datasets are provided by the Demand-Side Plateform (DSP) iPinYou Information Technologies Co., Ltd which is an established leading provider for audience-based programmatic adverting technology and builds RTB technology and algorithms. We examine the dataset of the advertiser with the advertiser ID \"1458\" that operates in the chinese vertical e-commerce industry. \n",
    "\n",
    "The data sets are split into train and test sets according according to Zhang et al. (2014), using their [Github repository](https://github.com/wnzhang/make-ipinyou-data), which is considered a benchmark format of the iPinYou dataset and also used for training and evaluating the DRLB framework. For the train set the first seven days are used for training, while for the test dataset the last three days are used for evaluation. Hence, in total the datasets comprise tracked performance from ten continous days.\n",
    "\n",
    "Both data sets generally record the auction price (i.e. paying and bidding price), user feedback (i.e click, conversion, impression) and ad features (e.g. ad slot ID, width, height, visibility, floor price). In this dataset we deal with the **second price auction**, which means that the varaible *payprice*, representing the highest bid of \"opponents\" bidding simuiltaneously on a given ad placement, determines how much in the end will be paid for the bid request.\n",
    "\n",
    "Another important aspect is that we slightly modified each dataset by including additional variables, in accordance with Wu et al (2018). We added the predicted click-trough-rate (pCTR) and time intervals for every quarter of an hour in minutes (min). The pCTR is used as an indicator of the impression value (i.e. how much is a given ad placement worth - the higher the probability of the click, the more it is worth to the advertiser), which is a second parameter ($v_i$) used to calculate the bid amount for a given ad placement, as explained in the introduction. Each 15 min interval represents a time step in the DRLB framework.\n",
    "\n",
    "As for the variable pCTR, we applied a logistic regression to predict the CTR based on the features of the train set and clicks as our target variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you should find yourself inside the project folder after you've pulled the latest changes\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_bid_train = pd.read_csv('data/ipinyou/1458/train.log.txt', sep=\"\\t\") \n",
    "df_bid_test = pd.read_csv('data/ipinyou/1458/test.log.txt', sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3083056 entries, 0 to 3083055\n",
      "Data columns (total 29 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   click           int64  \n",
      " 1   pCTR            float64\n",
      " 2   weekday         int64  \n",
      " 3   hour            int64  \n",
      " 4   min             int64  \n",
      " 5   bidid           object \n",
      " 6   timestamp       int64  \n",
      " 7   logtype         int64  \n",
      " 8   ipinyouid       object \n",
      " 9   useragent       object \n",
      " 10  IP              object \n",
      " 11  region          int64  \n",
      " 12  city            int64  \n",
      " 13  adexchange      int64  \n",
      " 14  domain          object \n",
      " 15  url             object \n",
      " 16  urlid           float64\n",
      " 17  slotid          object \n",
      " 18  slotwidth       int64  \n",
      " 19  slotheight      int64  \n",
      " 20  slotvisibility  int64  \n",
      " 21  slotformat      int64  \n",
      " 22  slotprice       int64  \n",
      " 23  creative        object \n",
      " 24  bidprice        int64  \n",
      " 25  payprice        int64  \n",
      " 26  keypage         object \n",
      " 27  advertiser      int64  \n",
      " 28  usertag         object \n",
      "dtypes: float64(2), int64(17), object(10)\n",
      "memory usage: 682.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# This is an overiew of the fratures in the training data\n",
    "df_bid_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the train data is (3083056, 29)\n",
      "The data set has 3083056 cases.\n",
      "The total number of elements is 89408624.\n",
      "The total numbers of missing values of each variable in train data are \n",
      "click                   0\n",
      "pCTR                    0\n",
      "weekday                 0\n",
      "hour                    0\n",
      "min                     0\n",
      "bidid                   0\n",
      "timestamp               0\n",
      "logtype                 0\n",
      "ipinyouid               0\n",
      "useragent               0\n",
      "IP                      0\n",
      "region                  0\n",
      "city                    0\n",
      "adexchange              0\n",
      "domain             169140\n",
      "url                 68566\n",
      "urlid             3083056\n",
      "slotid                  0\n",
      "slotwidth               0\n",
      "slotheight              0\n",
      "slotvisibility          0\n",
      "slotformat              0\n",
      "slotprice               0\n",
      "creative                0\n",
      "bidprice                0\n",
      "payprice                0\n",
      "keypage                 0\n",
      "advertiser              0\n",
      "usertag            399463\n",
      "dtype: int64.\n"
     ]
    }
   ],
   "source": [
    "# View dimension and missing values of the train set\n",
    "#print(df_bid_train.columns)\n",
    "print('Dimensionality of the train data is {}'.format(df_bid_train.shape)) \n",
    "print('The data set has {} cases.'.format(df_bid_train.shape[0]))    \n",
    "print('The total number of elements is {}.'.format(df_bid_train.size))\n",
    "print('The total numbers of missing values of each variable in train data are \\n{}.'.format(df_bid_train.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614638 entries, 0 to 614637\n",
      "Data columns (total 31 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   click           614638 non-null  int64  \n",
      " 1   pCTR            614638 non-null  float64\n",
      " 2   weekday         614638 non-null  int64  \n",
      " 3   hour            614638 non-null  int64  \n",
      " 4   min             614638 non-null  int64  \n",
      " 5   bidid           614638 non-null  object \n",
      " 6   timestamp       614638 non-null  int64  \n",
      " 7   logtype         614638 non-null  int64  \n",
      " 8   ipinyouid       614638 non-null  object \n",
      " 9   useragent       614638 non-null  object \n",
      " 10  IP              614638 non-null  object \n",
      " 11  region          614638 non-null  int64  \n",
      " 12  city            614638 non-null  int64  \n",
      " 13  adexchange      614638 non-null  int64  \n",
      " 14  domain          582360 non-null  object \n",
      " 15  url             604063 non-null  object \n",
      " 16  urlid           0 non-null       float64\n",
      " 17  slotid          614638 non-null  object \n",
      " 18  slotwidth       614638 non-null  int64  \n",
      " 19  slotheight      614638 non-null  int64  \n",
      " 20  slotvisibility  614638 non-null  int64  \n",
      " 21  slotformat      614638 non-null  int64  \n",
      " 22  slotprice       614638 non-null  int64  \n",
      " 23  creative        614638 non-null  object \n",
      " 24  bidprice        614638 non-null  int64  \n",
      " 25  payprice        614638 non-null  int64  \n",
      " 26  keypage         614638 non-null  object \n",
      " 27  advertiser      614638 non-null  int64  \n",
      " 28  usertag         519678 non-null  object \n",
      " 29  nclick          614638 non-null  int64  \n",
      " 30  nconversation   614638 non-null  int64  \n",
      "dtypes: float64(2), int64(19), object(10)\n",
      "memory usage: 145.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#'This is the feature description of the test data.'\n",
    "df_bid_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3080602\n",
       "1       2454\n",
       "Name: click, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency of clicks\n",
    "df_bid_train.click.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-54b10ca03074>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# amount of impressions bases on regions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_bid_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_bid_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'click'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'quicksort'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'amount of clicks'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# amount of impressions bases on regions\n",
    "\n",
    "plt.figure(1)\n",
    "df_bid_train[df_bid_train['click']==1].region.value_counts().sort_index(axis=0,  kind='quicksort').plot(kind='bar') \n",
    "plt.ylabel('amount of clicks')\n",
    "plt.xlabel('region')\n",
    "\n",
    "plt.figure(2)\n",
    "df_bid_train[df_bid_train['click']==0].region.value_counts().sort_index(axis=0,  kind='quicksort').plot(kind='bar') \n",
    "plt.ylabel('amount of impression')\n",
    "plt.xlabel('region')\n",
    "plt.show()\n",
    "\n",
    "# the impression in region 395 is always lose-impression\n",
    "print(\"the impression in region 395 is always lose-impression and the amount of impression in region 395 is: \", len(df_bid_train[(df_bid_train['click']==0) & (df_bid_train['region']==395)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# amount of impressions group by hours\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "df_bid_train[df_bid_train['click']==1].hour.value_counts().sort_index(axis=0,  kind='quicksort').plot(kind='bar') \n",
    "plt.ylabel('amount of clicks')\n",
    "plt.xlabel('hours')\n",
    "plt.figure(2)\n",
    "df_bid_train[df_bid_train['click']==0].hour.value_counts().sort_index(axis=0,  kind='quicksort').plot(kind='bar') \n",
    "plt.ylabel('amount of impressions')\n",
    "plt.xlabel('hours')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# the click amount rises and falls among time obviously， for example at 12 ,17 and 21 o'clock, user are more active to click the ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>android</th>\n",
       "      <td>19854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrome</th>\n",
       "      <td>926578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firefox</th>\n",
       "      <td>22853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ie</th>\n",
       "      <td>2003772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ios</th>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linux</th>\n",
       "      <td>1740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mac</th>\n",
       "      <td>64824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxthon</th>\n",
       "      <td>7675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opera</th>\n",
       "      <td>3226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>33685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>safari</th>\n",
       "      <td>72182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sogou</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theworld</th>\n",
       "      <td>17536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windows</th>\n",
       "      <td>2991649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          impression amount\n",
       "android               19854\n",
       "chrome               926578\n",
       "firefox               22853\n",
       "ie                  2003772\n",
       "ios                     509\n",
       "linux                  1740\n",
       "mac                   64824\n",
       "maxthon                7675\n",
       "opera                  3226\n",
       "other                 33685\n",
       "safari                72182\n",
       "sogou                    29\n",
       "theworld              17536\n",
       "windows             2991649"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of payprice\n",
    "\n",
    "CPC_mean=df_bid_train[df_bid_train['click']==1].payprice.mean()\n",
    "CPM_mean=df_bid_train[df_bid_train['click']==0].payprice.mean()\n",
    "print(\"the mean payprice for click is:{}\\nthe mean payprice for unclick is: {} \".format(CPC_mean,CPM_mean))\n",
    "\n",
    "plt.figure(1)\n",
    "df_bid_train[df_bid_train['click']==1].payprice.plot(kind='box', showfliers=False) \n",
    "\n",
    "plt.figure(2)\n",
    "df_bid_train[df_bid_train['click']==0].payprice.plot(kind='box', showfliers=False) \n",
    "\n",
    "plt.show()\n",
    "# the average costs: CPC (if clicked), CPM (if not clicked) \n",
    "CPC_mean=df_bid_train[df_bid_train['click']==1].payprice.mean()\n",
    "CPM_mean=df_bid_train[df_bid_train['click']==0].payprice.mean()\n",
    "print(\"the mean payprice for click is:{}\\nthe mean payprice for unclick is: {} \".format(CPC_mean,CPM_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional variable min \n",
    "\n",
    "As we mentioned before, each 15 min interval represents a time step in the DRLB framework. Therefore, we extract minute interval with values [\"00\",\"15\",\"30\",\"45\"] from timestamp as an additional variable \"min\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the min intervals\n",
    "\n",
    "def get_time_interval(data):\n",
    "    time_inv=int(data[10:12])\n",
    "    if time_inv>=0 and time_inv<15:\n",
    "            return (\"00\")\n",
    "    elif time_inv >= 15 and time_inv < 30:\n",
    "            return (\"15\")\n",
    "    elif time_inv >= 30 and time_inv < 45:\n",
    "            return (\"30\")\n",
    "    elif time_inv >= 45 and time_inv <=60:\n",
    "            return (\"45\")\n",
    "    else:\n",
    "            return(None)\n",
    "\n",
    "if 'min' in df_bid_train.columns:\n",
    "    print(\"min already exists\")\n",
    "    pass\n",
    "else:\n",
    "    df_bid_train[\"timestamp\"]=df_bid_train[\"timestamp\"].apply(str)\n",
    "    min_intervals = df_bid_train.apply(lambda row : get_time_interval(row['timestamp']), axis = 1)\n",
    "    df_bid_train.insert(3, \"min\", min_intervals) # insert the new column after the 'hour' column\n",
    "    # save the updated dataset\n",
    "    # df_bid_train.to_csv('data/ipinyou/1458/train.log.txt', sep=\"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### logistic regression for CTR\n",
    " \n",
    " The following briefly explains the preprocessing of the train data to obtain the predicted CTRs which is based on the click of the train set as our target variable.   \n",
    "\n",
    "To estimate the CTR and therefore to predict the probability of a user to click on an ad impression, we apply the logistic regression model as our prediction model. In the logistic regression models most of our features are dummies. Thus, most of these features we converted into categorical features first and then transformed them into dummy variables.\n",
    "\n",
    "Also, we briefly want to highlight some of the (dummy) feature transformations more in detail. From user agent we exctracted the browser and operation systems. User tags are processed to extract the tag list of each user. According to Zhang et al. (2014) the weekday and hour feature are extracted from the timestamps. As mentioned earlier, we additionally extracted time intervals for every quarter points of an hour in minutes. Furthermore, region, city, ad slot width, height, visibility, format, ad exchange are processed in the logistic regression model as dummy variables and the ad slot floor price as numeric. In total, we have 312 binary features for the logistic regression model.   \n",
    "Overall, we included features, which are quite generic and not fixed or unique values such as IDs, URLs or paying price.   \n",
    "\n",
    "In the logistic regression model the default of L2 regularization is used. Also, we want to emphasize that a significant imbalance of 0 (no clicks) and 1 (clicks) values exists in the target variable. Hence, we balanced the minority class 1 by using the class_weight parameter of the logistic regression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pCTR AUC in the train set:  0.9452306887829061\n",
      "pCTR AUC in the test set:  0.9735301800409389\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"pCTR AUC in the train set: \", roc_auc_score(df_bid_train['click'], df_bid_train['pCTR']))\n",
    "print(\"pCTR AUC in the test set: \", roc_auc_score(df_bid_test['click'], df_bid_test['pCTR']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, our prediction model yields a very high prediction accuracy, and especially interesting is the fact the AUC in the test set is higher than in the train set. We double-checked our logistic regression and implementation and couldnt't find any data leakage from the test set or the target into the fitted model.  \n",
    "We assume that the high prediction value results from the user tag variable. The user tag variable basically is the segmentation of users into different groups according to heterogeneous features within the group and targeting each group with ads. User segmentation can be quite important for predicting the user behavior response on ads. Different CTRs can be received on different user tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training Logitstic Regression model and predict the CTR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "class Logistic_Regression():\n",
    "    \n",
    "    # The dummified datasets is not proviuded, due to their big size (>5GB)\n",
    "    df_bid_requests=pd.read_csv(\".../dummified_train_test.txt\", sep=\"\\t\")\n",
    "    df_bid_test=pd.read_csv(\"...//dummified_test_test.txt\", sep=\"\\t\")\n",
    "    \n",
    "    #training\n",
    "\n",
    "    df_bid_test=pd.DataFrame(df_bid_test.astype(np.uint8))\n",
    "    df_bid_requests=pd.DataFrame(df_bid_requests.astype(np.uint8))\n",
    "\n",
    "    X = df_bid_requests.drop(['click','slotprice'], axis=1)\n",
    "    Y = df_bid_requests.click\n",
    "    df_bid_test=df_bid_test.drop(['slotprice'],axis=1)\n",
    "\n",
    "    #df_bid_test = df_bid_test[np.intersect1d(X.columns, df_bid_test.columns)]\n",
    "\n",
    "    X=X[np.intersect1d(X.columns, df_bid_test.columns)]\n",
    "\n",
    "############ Train model with Logitstic Regression\n",
    "    #Logistic Regression: Model fitting\n",
    "    \n",
    "    logreg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    logreg.fit(X, Y)\n",
    "\n",
    "    #save models\n",
    "    joblib_filename = \"logreg.sav\"\n",
    "    joblib.dump(logreg, joblib_filename)\n",
    "\n",
    "\n",
    "    # Check auccuracy score\n",
    "    print(logreg.score(X,Y))\n",
    "    prob = logreg.predict_proba(X)[:, 1]\n",
    "    print(roc_auc_score(Y, prob))\n",
    "\n",
    "    logreg = joblib.load(\"logreg.sav\")\n",
    "    #\n",
    "    # # ## Logistic Regression: Predict CTR\n",
    "    ctr_pre_train=logreg.predict(X)\n",
    "    ctr_pre_train_proba=logreg.predict_proba(X)\n",
    "    df_ctr_train = pd.DataFrame(ctr_pre_train_proba, columns=['ctr_prediction_pro','ctr_prediction'])\n",
    "    df_ctr_train['click'] =ctr_pre_train\n",
    "    df_ctr_train.to_csv('.../ctr_pred_train.txt', index=False, sep='\\t', header=True)\n",
    "\n",
    "    ctr_pred = logreg.predict(df_bid_test[X.columns])\n",
    "    ctr_pre_proba = logreg.predict_proba(df_bid_test[X.columns])\n",
    "\n",
    "\n",
    "    df_ctr_test = pd.DataFrame(ctr_pre_proba, columns=['ctr_prediction_pro_0', 'ctr_prediction_pro_1'])\n",
    "    df_ctr_test['click']=ctr_pred\n",
    "    print(df_bid_test)\n",
    "    df_ctr_test.to_csv('.../ctr_pred_test.txt',\n",
    "                         index=False, sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. DRLB Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the implementation of DRLB was not made public by the authors, we had to code it on our own. Although we've found [one implementation](https://github.com/venkatacrc/Budget_Constrained_Bidding) of DRLB on Github, it is missing some functionality that was introduced in the paper, and, to be honest, has some parts that are flat out wrongly implemented. Therefore, even though this code was very useful as a skeleton to base our implementation on, we've added a lot of fixes and improvements to it. \n",
    "\n",
    "Since the codebase is quite large to use directly in the notebook, we'll include here only some important parts. Let's first start by looking at how each state $s_t$ state in the (episodic) CMDP formulated for the RTB problem is defined. Remember, one episode is represented by one day in the iPinYou data, while one time step is equal to a 15 min interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_state(self):\n",
    "    \"\"\"\n",
    "    Returns the state that will be used as input in the DQN\n",
    "    \"\"\"\n",
    "    return np.asarray([self.t_step, # 1. Current time step\n",
    "            self.rem_budget, # 2. the remaining budget at time step t\n",
    "            self.ROL, # 3. The number of Lambda regulation opportunities left in the episode (amount of time steps left)\n",
    "            self.BCR, # 4. Budget consumption rate: (Bt - Bt-1)/Bt\n",
    "            self.CPM, # 5. Cost per mille of impressions between t-1 and t: (costs at t/impressions at t)*1000\n",
    "            self.WR, # 6. Auction win rate at state t: (won impressions at t)/(total bidding opportunities at t)\n",
    "            self.rewards_prev_t]) # 7. Total clicks on the ad placements we've won at t-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how the bidding agent that acts in our problem's environment, based on the states that it receives, is initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RlBidAgent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Beta parameter adjusting lambda, that regulates the agent's bid amount\n",
    "        self.BETA = [-0.08, -0.03, -0.01, 0, 0.01, 0.03, 0.08]\n",
    "        # Starting value of epsilon in the adaptive epsilon-greedy policy\n",
    "        self.eps = 0.9\n",
    "        # Parameter controlling the annealing speed of epsilon, suggested by Wu et al. (2018)\n",
    "        self.anneal = 2e-5\n",
    "        # DQN Network to learn Q function\n",
    "        self.dqn_agent = DQN(state_size = 7, action_size = len(self.BETA))\n",
    "        # Reward Network to learn the reward function\n",
    "        self.reward_net = RewardNet(state_action_size = len(self.BETA) + 1, reward_size = 1)\n",
    "        # Number of timesteps in each episode (4 15min intervals x 24 hours = 96), as formulated by Wu et al. (2018)\n",
    "        self.T = 96\n",
    "        # Initialize the DQN action for t=0 (index 3 - no adjustment of lambda, 0 in self.BETA)\n",
    "        self.dqn_action = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first beta parameter is basically all the actions that the DQN is able to take at each time step to control the lambda parameter, thus corresponding to the action size (i.e. the output layer) of the DQN. It outputs the distribution of Q values over all actions, so e.g. if the highest Q value is at index 3 of the output layer, the bidding agent adjusts lambda by 0 percent:\n",
    "\n",
    "$$ \\lambda_{t-1} = \\lambda_t \\times (1 + \\beta_a). $$\n",
    "\n",
    "The RewardNet takes the state-action pair as input, while outputting one reward value. Both networks consist of 3 hidden layers with 100 neurons in each layer and ReLU activation functions. Here's how each of the networks is initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 32         # minibatch size\n",
    "GAMMA = 1.0             # discount factor\n",
    "LR = 1e-3               # learning rate \n",
    "C = 100                 # how often to update the network\n",
    "\n",
    "class DQN():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        set_seed()\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = Network(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = Network(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for using two differnet instantiations of the Q network simultaneously was explained by Minh et al. (2015), who introduced the DQN itself. Every $C$ updates the `qnetwork_local` is cloned to obtain `qnetwork_target`, which is used for generating the Q-learning targets $y$ for the following $C$ updates to `qnetwork_local`. This makes the algorithm more stable, since generating the targets using an older set of parameters adds a delay between the time an update to `qnetwork_local` is made and the time the update affects the targets $y$, making divergence or oscillations of the learned policy much more unlikely. The input to the `qnetwork_local` is the current state $s_t$, while the `qnetwork_target` receives state $s_{t+1}$ as input.\n",
    "\n",
    "To get an intuitive understanding for why the targets for predictions of the DQN for $s_t$ are predictions of (almost) the same DQN for $s_{t+1}$, think about it as a way to peek into the future using all available resources at the current time step. Maybe knowing how the DQN performs an update of its parameters will aid in understanding this idea a bit better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, experiences, gamma, terminal):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s') tuples \n",
    "            gamma (float): discount factor, always equal to 1 in DRLB\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states = experiences\n",
    "\n",
    "        if terminal == 1: # if the next state is the last one in the episode\n",
    "            y = rewards\n",
    "        else:\n",
    "            y = rewards + gamma * self.qnetwork_target(next_states).max(1, keepdim=True)[0] # calculate best actions for each of the 32 s_t+1\n",
    "\n",
    "        # Get Q values from local model\n",
    "        Q_local = self.qnetwork_local(states).gather(1, actions) # get the Q values of the current DQN for the hsitorical actions taken\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.criterion(Q_local, y)\n",
    "        print(\"DQN loss = {}\".format(loss))\n",
    "        # Grad descent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.loss = loss.item()\n",
    "        # Every C steps reset Q target = Q (hard copy)\n",
    "        if ((self.t_step + 1) % C) == 0:\n",
    "            for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
    "                target_param.data.copy_(local_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every $C=100$ steps the DQN samples a mini-batch containing 32 of each (corresponding to each other) $s_t$, $s_{t+1}$, $r_t$ and $a_t$ from its replay memory (initialzied above with maximum length of 100 000 records/states). So, the action of DQN in state $s_t$ is evaluated based on the action that it would take at the next step $s_{t+1}$, based on the reward for the action at the current step. And this chain can be unfolded for each time step, except for the last one in the episode. \n",
    "\n",
    "The gamma parameter is always set to 1 in DRLB, since the optimization goal of the budget constrained bidding problem is to maximize the total reward value under the cost constraint regardless of the reward time (i.e. we don't want to punnish getting clicks on the won ad placements later in the episode).\n",
    "\n",
    "But how does the DQN decide on which actions to make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(self, state, eps, eval_flag):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)[0] # [0] 'cause otherwise nested array\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if eval_flag == 0:\n",
    "            # Epsilon-greedy action selection\n",
    "            # Check if the Q-value distribution is unimodal, if so:\n",
    "            if self.unimodal_check(action_values) == True:\n",
    "                if random.random() <= eps:\n",
    "                    # choose action randomly with prob epsilon\n",
    "                    return random.choice(np.arange(self.action_size))\n",
    "                else: # and a regular action with 1-eps\n",
    "                    return np.argmax(action_values.cpu().data.numpy())\n",
    "            # If not unimodal, increase epsilon, if it's small\n",
    "            else:\n",
    "                prob = max(eps, 0.5)\n",
    "                if random.random() <= prob:\n",
    "                    return random.choice(np.arange(self.action_size))\n",
    "                else: # and with 1-p choose an action regularly \n",
    "                    return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return np.argmax(action_values.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unimodality check of the action-value distribution $Q(s_t, a_t)$, was proposed by the authors of DRLB - theyu believe that when the distribution is not unimodal, the current estimation of Q is abnormal and the $\\epsilon$ should be increased to encourage more explorations under this state. The following figure taken from the DRLB paper illustrates the unimodal (normal) vs abnormal distribution. The unimodality check is done using a simple heuristic test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Q_distribution.png\" width=750 height=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the full training algorithms of the DQN and the RewardNet. The training process of the RewardNet is much simpler, since it simply learns to predict an optimal reward for a given state-action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/DRLB.png\" width=900 height=900 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\lambda_0$ at the beginning of each episode is calculated by solving the bidding problem as a knapsack problem using the greedy approximation algorithm introduced by Dantzig (1957)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_greedy(items, budget_limit):\n",
    "    # Borrowed from: https://bitbucket.org/trebsirk/algorithms/src/master/knapsack.py\n",
    "    bids = []\n",
    "    spending = 0\n",
    "    ctr = 0\n",
    "    items_sorted = sorted(items, key=itemgetter(2), reverse=True) # sort bid requests by pCTR/payprice\n",
    "    while len(items_sorted) > 0:\n",
    "        item = items_sorted.pop()\n",
    "        if item[0] + spending <= budget_limit:\n",
    "            bids.append(item)\n",
    "            spending += bids[-1][1]\n",
    "            ctr += bids[-1][0]\n",
    "        else:\n",
    "            break\n",
    "    ctrs = np.array(bids)[:,0]\n",
    "    costs = np.array(bids)[:,1]\n",
    "    # Take the max lambda to be more conservative at the beginning of a time step\n",
    "    opt_lambda = np.max(np.divide(ctrs, costs))\n",
    "    return opt_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DRLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neither Wu et al. (2018), nor Cai et al. (2017), against whose framework the former compared DRLB, provide a budget constraint during training, we decided to initialize the budget for each episode as the total cost of the auction (sum of all payprices) divided by 4 and split into 7 equal parts. We thought that this may be a good balance between giving an agent enough money play around with, while also being cosntrained enough.\n",
    "\n",
    "Warning: the DRLB takes a long time, therefore we commented out the training code, so you don't accidentally run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train DRLB\n",
    "\n",
    "# import sys  \n",
    "# import torch\n",
    "# import cloudpickle\n",
    "# sys.path.insert(0, 'src/rtb_agent')\n",
    "# from rl_bid_agent import RlBidAgent\n",
    "# from model import set_seed\n",
    "# import gym, gym_auction_emulator\n",
    "\n",
    "# #Instantiate the Environment and Agent\n",
    "# env = gym.make('AuctionEmulator-v0')\n",
    "# env.seed(0)\n",
    "# set_seed()\n",
    "# agent = RlBidAgent()\n",
    "\n",
    "# epochs = 200\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     print(\"Epoch: \", epoch+1)\n",
    "\n",
    "#     obs, done = env.reset()\n",
    "#     train_budget = env.bid_requests.payprice.sum()/4\n",
    "#     # Set budgets for each episode\n",
    "#     budget_proportions = []\n",
    "#     for episode in env.bid_requests.weekday.unique():\n",
    "#         budget_proportions.append(len(env.bid_requests[env.bid_requests.weekday == episode])/env.total_bids)\n",
    "#     for i in range(len(budget_proportions)):\n",
    "#         budget_proportions[i] = round(train_budget * budget_proportions[i])\n",
    "#     agent.episode_budgets = budget_proportions\n",
    "    \n",
    "#     agent._reset_episode()\n",
    "#     agent.cur_day = obs['weekday']\n",
    "#     agent.cur_hour = obs['hour']\n",
    "#     agent.cur_state = agent._get_state() # observe state s_0\n",
    "\n",
    "#     while not done: # iterate through the whole dataset\n",
    "#         bid = agent.act(obs) # Call agent action given each bid request from the env\n",
    "#         next_obs, cur_reward, cur_cost, win, done = env.step(bid) # Get information from the environment based on the agent's action\n",
    "#         agent._update_reward_cost(bid, cur_reward, cur_cost, win) # Agent receives reward and cost from the environment\n",
    "#         obs = next_obs\n",
    "#     print(\"Episode Result with Step={} Budget={} Spend={} wins={} rewards={}\".format(agent.global_T, agent.budget, agent.budget_spent_e, agent.wins_e, agent.rewards_e))\n",
    "#     agent.episode_memory.append([agent.budget, agent.budget_spent_e, agent.wins_e, agent.rewards_e])\n",
    "\n",
    "#     # Saving models and history\n",
    "#     if ((epoch + 1) % 10) == 0:\n",
    "#         PATH = 'models/model_state_{}.tar'.format(epoch+1)\n",
    "#         torch.save({'local_q_model': agent.dqn_agent.qnetwork_local.state_dict(),\n",
    "#                     'target_q_model':agent.dqn_agent.qnetwork_target.state_dict(),\n",
    "#                     'q_optimizer':agent.dqn_agent.optimizer.state_dict(),\n",
    "#                     'rnet': agent.reward_net.reward_net.state_dict(),\n",
    "#                     'rnet_optimizer': agent.reward_net.optimizer.state_dict()}, PATH)\n",
    "\n",
    "#         f = open('models/rnet_memory_{}.txt'.format(epoch+1), \"wb\")\n",
    "#         cloudpickle.dump(agent.dqn_agent.memory, f)\n",
    "#         f.close()\n",
    "#         f = open('models/rdqn_memory_{}.txt'.format(epoch+1), \"wb\")\n",
    "#         cloudpickle.dump(agent.reward_net.memory, f)\n",
    "#         f.close()\n",
    "\n",
    "#         pd.DataFrame(agent.step_memory).to_csv('models/step_history_{}.csv'.format(epoch+1),header=None,index=False)\n",
    "#         agent.step_memory=[]\n",
    "#         pd.DataFrame(agent.episode_memory).to_csv('models/episode_history_{}.csv'.format(epoch+1),header=None,index=False)\n",
    "#         agent.episode_memory=[]\n",
    "\n",
    "#     print(\"EPOCH ENDED\")\n",
    "\n",
    "# env.close() # Close the environment when done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Random Bidding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline for our DRLB implementation we decided to first use just simple random uniform bidding. As a budget constraint Cai et al. (2017) provide the following calculation:\n",
    "\n",
    "$$B = CPM_{train} \\times 10^{-3} \\times T \\times c_0$$\n",
    "\n",
    "where $CPM_{train}$ is the cost per mille of impressions in the training data, $T$ is the amount of steps in each episode and $c_0$ acts as the budget constraint parameter. Following their work, we set multiple values of $c_0$ = 1/32, 1/16, 1/8, 1/4, 1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datasets\n",
    "import pandas as pd\n",
    "bid_requests_train = pd.read_csv('data/ipinyou/1458/train.log.txt', sep=\"\\t\")\n",
    "bid_requests_test = pd.read_csv('data/ipinyou/1458/test.log.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bugdet</th>\n",
       "      <th>portion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3307.0</td>\n",
       "      <td>1/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1653.0</td>\n",
       "      <td>1/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>827.0</td>\n",
       "      <td>1/8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>413.0</td>\n",
       "      <td>1/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207.0</td>\n",
       "      <td>1/32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bugdet portion\n",
       "0  3307.0     1/2\n",
       "1  1653.0     1/4\n",
       "2   827.0     1/8\n",
       "3   413.0    1/16\n",
       "4   207.0    1/32"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define budget for each episode according to (Cai et al., 2017)\n",
    "CPM_train = 1000 * (bid_requests_train.payprice.sum()/len(bid_requests_train))\n",
    "budget = CPM_train * (10**-3) * 96\n",
    "episode_budgets = [budget/2,budget/4,budget/8,budget/16,budget/32]\n",
    "bid_random = bid_requests_test\n",
    "bid_budget = pd.DataFrame()\n",
    "bid_budget['bugdet']=episode_budgets\n",
    "bid_budget['portion']=['1/2','1/4','1/8','1/16','1/32']\n",
    "bid_budget.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Bidding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# create a temporary dataframe\n",
    "c_names=('click','day', 'slotprice', 'payprice','r_bid_amt','wins')\n",
    "zero_Data=np.zeros(shape=(len(bid_requests_test),len(c_names)))\n",
    "df=pd.DataFrame(zero_Data,columns=c_names)\n",
    "df['day']=bid_requests_test['weekday']\n",
    "df['click']=bid_requests_test['click']\n",
    "df['slotprice'] = bid_requests_test['slotprice']\n",
    "df['payprice'] = bid_requests_test['payprice']\n",
    "# generate random bids for each bid request, no higher than the bids of other auction participants\n",
    "df['r_bid_amt']=np.random.randint(0,bid_requests_test['payprice'].max(),len(bid_requests_test))\n",
    "\n",
    "i = 1\n",
    "for budget in episode_budgets:\n",
    "    for day in df['day'].unique(): # for each episode\n",
    "        episode = df[df['day'] == day].copy(deep = True) # get an episode subset from the whole test set\n",
    "        rem_budget = budget # and set the budget\n",
    "        while rem_budget > 0: # while the budget is not exhausted\n",
    "            bid_request = episode.sample() # sample a random bid request\n",
    "            if bid_request['r_bid_amt'].values[0] > bid_request['payprice'].values[0]:\n",
    "                index = bid_request.index[0]\n",
    "                episode = episode.drop(episode.loc[episode.index == index].index[0]) # remove bid request if we win the auction\n",
    "                df.loc[index, 'wins'] = i # track which budget won the auction\n",
    "                # adjust the budget accordingly\n",
    "                rem_budget = rem_budget - max(bid_request['payprice'].values[0], bid_request['slotprice'].values[0])\n",
    "                # stop if not possible to win anything anymore\n",
    "                if rem_budget <= episode['payprice'].min():\n",
    "                    rem_budget = 0\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_clicks = []\n",
    "rand_imps = []\n",
    "for i in range (4,7):\n",
    "    for j in range(1,6):\n",
    "        rand_clicks.append(df[(df['wins'] == j) & (df['day'] == i)]['click'].sum())\n",
    "        rand_imps.append(len(df[(df['wins'] == j) & (df['day'] == i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of random bidding on the test set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>day 8</th>\n",
       "      <th>day 9</th>\n",
       "      <th>day 10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reward</th>\n",
       "      <th>portion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">clicks</th>\n",
       "      <th>1/2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">impression</th>\n",
       "      <th>1/2</th>\n",
       "      <td>61</td>\n",
       "      <td>59</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/4</th>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/8</th>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/16</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/32</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    day 8  day 9  day 10\n",
       "reward     portion                      \n",
       "clicks     1/2          0      0       0\n",
       "           1/4          0      0       0\n",
       "           1/8          0      0       0\n",
       "           1/16         0      0       0\n",
       "           1/32         0      0       0\n",
       "impression 1/2         61     59      63\n",
       "           1/4         32     30      29\n",
       "           1/8          9     14      15\n",
       "           1/16         7      6       5\n",
       "           1/32         3      4       3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results of random bidding\n",
    "\n",
    "interables=[['clicks','impression'],['1/2','1/4','1/8','1/16','1/32']]\n",
    "arrays=pd.MultiIndex.from_product(interables, names=['reward', 'portion'])\n",
    "a=pd.Series(rand_clicks[:5] + rand_imps[:5])\n",
    "b=pd.Series(rand_clicks[5:10] + rand_imps[5:10])\n",
    "c=pd.Series(rand_clicks[10:] + rand_imps[10:])\n",
    "display=pd.DataFrame(pd.concat([a,b,c],axis=1).values, index=arrays)\n",
    "display.columns=[\"day 8\",\"day 9\",\"day 10\"]\n",
    "print(\"Results of random bidding on the test set:\")\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DRLB Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained the DRLB for approx. 200.000 time steps. Wu et al. (2018) suggested at least 125.000 and ideally 250.000 time steps. Let's take a look at how the trained model bids on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Result with Step=95 Budget=3306.8525411150495 Spend=3306.8525411150495 wins=1 rewards=0\n",
      "Episode Result with Step=189 Budget=3306.8525411150495 Spend=3306.8525411150495 wins=1 rewards=0\n",
      "Episode Result with Step=283 Budget=3306.8525411150495 Spend=3306.8525411150495 impressions=1 clicks=0\n",
      "Episode Result with Step=95 Budget=1653.4262705575247 Spend=1653.4262705575247 wins=1 rewards=0\n",
      "Episode Result with Step=189 Budget=1653.4262705575247 Spend=1653.4262705575247 wins=1 rewards=0\n",
      "Episode Result with Step=283 Budget=1653.4262705575247 Spend=1653.4262705575247 impressions=1 clicks=0\n",
      "Episode Result with Step=95 Budget=826.7131352787624 Spend=826.7131352787624 wins=1 rewards=0\n",
      "Episode Result with Step=189 Budget=826.7131352787624 Spend=826.7131352787624 wins=1 rewards=0\n",
      "Episode Result with Step=283 Budget=826.7131352787624 Spend=826.7131352787624 impressions=1 clicks=0\n",
      "Episode Result with Step=95 Budget=413.3565676393812 Spend=413.3565676393812 wins=1 rewards=0\n",
      "Episode Result with Step=189 Budget=413.3565676393812 Spend=413.3565676393812 wins=1 rewards=0\n",
      "Episode Result with Step=283 Budget=413.3565676393812 Spend=413.3565676393812 impressions=1 clicks=0\n",
      "Episode Result with Step=95 Budget=206.6782838196906 Spend=206.6782838196906 wins=1 rewards=0\n",
      "Episode Result with Step=189 Budget=206.6782838196906 Spend=206.6782838196906 wins=1 rewards=0\n",
      "Episode Result with Step=283 Budget=206.6782838196906 Spend=206.6782838196906 impressions=1 clicks=0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import cloudpickle\n",
    "sys.path.insert(0, 'src/rtb_agent')\n",
    "from rl_bid_agent import RlBidAgent\n",
    "from model import set_seed\n",
    "import gym, gym_auction_emulator\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make('AuctionEmulator-v0')\n",
    "env.seed(0)\n",
    "\n",
    "bid_requests_train = pd.read_csv('data/ipinyou/1458/train.log.txt', sep=\"\\t\")\n",
    "\n",
    "# Calculate lambda_0 for the first episode in the test set\n",
    "last_train_episode = bid_requests_train[bid_requests_train['weekday'] == 6]\n",
    "items = []\n",
    "for obs in last_train_episode.values:\n",
    "    items.append([obs[1], obs[25], obs[1]/max(obs[25], 1)])\n",
    "\n",
    "# Set the budgets for each episode according to Cai et al. (2017)\n",
    "CPM_train = 1000 * (bid_requests_train.payprice.sum()/len(bid_requests_train))\n",
    "budget = CPM_train * (10**-3) * 96\n",
    "eval_budgets = [budget/2,budget/4,budget/8,budget/16,budget/32]\n",
    "\n",
    "for e_budget in eval_budgets:\n",
    "\n",
    "    # Initialize the bidding agent\n",
    "    set_seed()\n",
    "    agent = RlBidAgent()\n",
    "\n",
    "    # Load the saved models\n",
    "    checkpoint = torch.load('models/model_state_40.tar')\n",
    "    agent.dqn_agent.qnetwork_local.load_state_dict(checkpoint['local_q_model'])\n",
    "    agent.dqn_agent.optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
    "    agent.reward_net.reward_net.load_state_dict(checkpoint['rnet'])\n",
    "    agent.reward_net.optimizer.load_state_dict(checkpoint['rnet_optimizer'])\n",
    "\n",
    "    # init the current budget for each episode\n",
    "    agent.episode_budgets = [e_budget, e_budget, e_budget]\n",
    "\n",
    "    # start evaluating\n",
    "    obs, done = env.reset()\n",
    "    agent._reset_episode()\n",
    "    agent.ctl_lambda = agent.calc_greedy(items, e_budget)\n",
    "    agent.cur_day = obs['weekday']\n",
    "    agent.cur_hour = obs['hour']\n",
    "    agent.cur_state = agent._get_state() # observe state s_0\n",
    "\n",
    "    while not done: # iterate through the whole dataset\n",
    "        bid = agent.eval_act(obs) # Call agent action given each bid request from the env\n",
    "        next_obs, cur_reward, cur_cost, win, done = env.step(bid) # Get information from the environment based on the agent's action\n",
    "        agent._update_reward_cost(bid, cur_reward, cur_cost, win) # Agent receives reward and cost from the environment\n",
    "        obs = next_obs\n",
    "    print(\"Episode Result with Step={} Budget={} Spend={} impressions={} clicks={}\".format(agent.global_T, agent.budget, agent.budget_spent_e, agent.wins_e, agent.rewards_e))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the performance of the DRLB is unsatisfiable to say the least. Basically the agent spends all its money on the first bid request it sees (and obviously wins it). Teh fact that we've trained the model for approx. 200.000 time steps (Wu et al. suggested at least 125.000 and ideally 250.000) tells us that the subpar results most likely stem not from the fact that the model wasn't trained long enough, but rather from some other weak part of the model.\n",
    "\n",
    "As a reminder, at the beginning of each episode the $\\lambda_0$ parameter is set as the result of solving the knapsack problem of the previouis episode with the greedy aproximation algorithm. Even by taking the highest lambda produced by that algorithm (which leads to lower bids), that lambda still produces much higher bids than the market price on a given ad placement, which leads to a very high spending in the first time step.\n",
    "\n",
    "To analyze this further, taking a look at the training history is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_T</th>\n",
       "      <th>rem_budget</th>\n",
       "      <th>ctl_lambda</th>\n",
       "      <th>eps</th>\n",
       "      <th>dqn_action</th>\n",
       "      <th>dqn_agent_loss</th>\n",
       "      <th>rnet_r</th>\n",
       "      <th>reward_net_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196109</td>\n",
       "      <td>4630614.86</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>7.341998e+06</td>\n",
       "      <td>25.252375</td>\n",
       "      <td>2.466341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196110</td>\n",
       "      <td>3654836.63</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>1.437010e+05</td>\n",
       "      <td>20.194681</td>\n",
       "      <td>0.223770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196111</td>\n",
       "      <td>3240797.59</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>1.188465e+04</td>\n",
       "      <td>20.194698</td>\n",
       "      <td>0.796758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196112</td>\n",
       "      <td>2710508.32</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>1.085120e+08</td>\n",
       "      <td>20.200081</td>\n",
       "      <td>3.149099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196113</td>\n",
       "      <td>2246655.13</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>4.880028e+06</td>\n",
       "      <td>20.217422</td>\n",
       "      <td>2.191170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>196196</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>2.049824e+07</td>\n",
       "      <td>26.953224</td>\n",
       "      <td>3.222082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>196197</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>4.409173e+06</td>\n",
       "      <td>26.988413</td>\n",
       "      <td>0.328590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>196198</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>2.046061e+03</td>\n",
       "      <td>27.015406</td>\n",
       "      <td>1.382332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>196199</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>3.612428e+07</td>\n",
       "      <td>26.984053</td>\n",
       "      <td>0.185301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>196200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>7.887514e+03</td>\n",
       "      <td>26.701885</td>\n",
       "      <td>1.236607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    global_T  rem_budget  ctl_lambda   eps  dqn_action  dqn_agent_loss  \\\n",
       "0     196109  4630614.86    0.000683  0.05           5    7.341998e+06   \n",
       "1     196110  3654836.63    0.000738  0.05           6    1.437010e+05   \n",
       "2     196111  3240797.59    0.000797  0.05           6    1.188465e+04   \n",
       "3     196112  2710508.32    0.000860  0.05           6    1.085120e+08   \n",
       "4     196113  2246655.13    0.000886  0.05           5    4.880028e+06   \n",
       "..       ...         ...         ...   ...         ...             ...   \n",
       "87    196196        0.00    0.000590  0.05           2    2.049824e+07   \n",
       "88    196197        0.00    0.000572  0.05           1    4.409173e+06   \n",
       "89    196198        0.00    0.000555  0.05           1    2.046061e+03   \n",
       "90    196199        0.00    0.000510  0.05           0    3.612428e+07   \n",
       "91    196200        0.00    0.000495  0.05           1    7.887514e+03   \n",
       "\n",
       "       rnet_r  reward_net_loss  \n",
       "0   25.252375         2.466341  \n",
       "1   20.194681         0.223770  \n",
       "2   20.194698         0.796758  \n",
       "3   20.200081         3.149099  \n",
       "4   20.217422         2.191170  \n",
       "..        ...              ...  \n",
       "87  26.953224         3.222082  \n",
       "88  26.988413         0.328590  \n",
       "89  27.015406         1.382332  \n",
       "90  26.984053         0.185301  \n",
       "91  26.701885         1.236607  \n",
       "\n",
       "[92 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import history of the first 25 epochs\n",
    "history1 = pd.read_csv('models/step_history_25.csv', names=['global_T', 'rem_budget', 'ctl_lambda', 'eps', \n",
    "                                                           'dqn_action', 'dqn_agent_loss', 'rnet_r', \n",
    "                                                           'reward_net_loss'])\n",
    "# Subset the last training episode from the 10th epoch\n",
    "last_train_episode_10 = history1[history1['global_T']>=16259].copy()\n",
    "last_train_episode_10 = last_train_episode_10.reset_index(drop=True)\n",
    "\n",
    "# Import history of the last 25 epochs\n",
    "history2 = pd.read_csv('models/step_history_300.csv', names=['global_T', 'rem_budget', 'ctl_lambda', 'eps', \n",
    "                                                           'dqn_action', 'dqn_agent_loss', 'rnet_r', \n",
    "                                                           'reward_net_loss'])\n",
    "# Subset the very last training episode\n",
    "last_train_episode = history2[history2['global_T']>=196109].copy()\n",
    "last_train_episode = last_train_episode.reset_index(drop=True)\n",
    "last_train_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb247e399d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzaklEQVR4nO3deXwU9f3H8dc7ISEcCWdALgkoh9wqRUCrIIIXHvXGqohWW6u1/qr1rtpaW9taK9YearVaT6wXrVoVK4h4IVjkRjyCct+EcCbh8/tjJriGHEvIZjabz/Px2Ae7M7Mz79ndfJj9zne/IzPDOedc6kmLOoBzzrnE8ALvnHMpygu8c86lKC/wzjmXorzAO+dcivIC75xzKcoLfB0n6UZJf6vpZWuTpEJJXWt4nVMkfa+az/2OpK/CXAfXZK7aJmmYpKW1vM0LJU2rzW3Wln35XEXBC3xIUr6kY2pwfVV+yGviw2JmvzKzuNaxN8vWJjNramafR50jxl3AFWGu/9XmhiVlSno2/DyapGFl5kvSbyStC2+/laSY+SbpwNrMnAiS8sJ9aRB1lrrMC3wS8w93ZDoD88qbUUvvyTTgPGBlOfMuBU4F+gP9gNHA92shk6uDvMBXQVILSS9JWiNpQ3i/Y8z8CyV9LmmzpC8kfVfSQcBfgSHh1/yN5az3DuDbwH3hMveF003S5ZIWA4vDaePDJoMCSTMlfTtmPbdJejy8X3rUM1bSl5LWSrqpmss2kvRouM8LJF1b2Vd9ST0lTZK0XtIiSWfFzHtE0l/D+ZslvSWpc8z83Uedkk6QND9cbpmka2KWu0TSp+E2/iWpfcy8kZIWStoUvpa7j2rD+ReF+7FB0mux249ZpqGkQiAd+FjSZ+H0fEnXSZoNbJHUQNLJkuZJ2hh+EzsoZj35kn4qabakLZIektRW0n/C/XpDUovyXkcz22lm95jZNKCknEXGAr83s6Vmtgz4PXBhuN2p4TIfh5+ps2MyXS1ptaQVksaVt21J50iaUWba/0n6V3i/wvemMlV8fgdJmhHOWyXp7nBW6b5sDPdlSDnrTZN0vaTPFHybeUZSy3Be6ef7UknLw/2+Oua5DSXdE85bHt5vGDP/FEmzwlyfSTouZtOdJb0Tvg6vS2odz+sQCTPzWzBcQz5wTDnTWwGnA42BbOCfwIvhvCZAAdAjfNwO6B3evxCYVsU2pwDfKzPNgElAS6BROO28MEcD4GqCI7uscN5twOPh/bzw+Q8CjQiO8nYAB1Vj2TuBt4AWQEdgNrC0gv1oAnwFjAszHgKsjXktHgE2A0cCDYHxsa9NmOPA8P4K4Nvh/RbAIeH9o8N1HhKu44/A1HBe6/B9OAPIAP4PKC59bQmOeD8FDgrz3Qy8W8n7sjtPzGdjFtApfK26A1uAkeH2rg3Xnxmz/PtAW6ADsBr4CDg4zP4mcGscn8mlwLAy0zYBh8U8HghsriT7sPC1+EWY9QRgK9CinO01Dt+nbjHTPgTOqey9KWc9F5Z5fyv7/L4HnB/ebwoMLvP5bFDJ63NV+Dp3DF/X+4Gnyjz/KYLPZ19gDeHfePh6vA+0AXKBd4Hbw3mDwtd5JMFBcAegZ8zf7GfhZ6BR+PjOqOtXha9R1AHKedMeDv8g5sa5/FnAfIKv1E/uw3bzKafAl7PcAGBDeL8JsJHgP4BGlX3IK1jXFMov8EdX8bwNQP/w/m3sWbQ7xiw7PeYPdG+W/Rw4Nmbe96i4wJ8NvF1m2v2ERYygwD8dM68pwdFpp5h9Li3wXxI0OeSUWd9DwG/LrKMo3I8LgPdj5omgOJYW+P8AF8fMTyMocp0r2J/yCvxFMY9/BjxTZn3LCItxuPx3Y+Y/B/wl5vGPCA8SqnifyyvwJYTFJnzcLcyrCrIPA7YRUygJ/r4GV7DNx4FbYta9GWhc2XtTzjoupJLPPt/8/E4Ffg60LrNM6eezsgK/ABgR87hd+JloEPP82Nfqt8BD4f3PgBNi5h0L5Md8dv9Qyd/szTGPfwi8WtV7GdUtGZtoHgGOq2ohAEndgBuAw82sN8H/6DVKUmNJ90taIqmA4APZXFK6mW0hKG4/AFZIellSzxrY7FdlMlwdNi9sUtDc04zgqLUisW23WwmK4d4u275Mjm9kKqMzcFjYXLExzPhdYL/ynm9mhcD6cBtlnU5wlLkkbMop/WreHlhSZh3rCI6uvpHVgr+82LydgfEx2dYT/CfQoZJ9Kit2fWWz7Arnx65vVcz9beU8ruw9qUwhkBPzOAcoDPe5IuvMrDjmcWWfiSeBMeH9cwn+I9oaPq7ovalUFZ/fiwmOhhdK+lDS6HjWGeoMvBDzvi4g+A+wbcwyse/bEr7+zH3jPSwzrxPBfwAV2Zu/r0glXYE3s6kEf4C7STpA0qth+93bMUX0EuBPZrYhfO7qBES6GuhB8LU4h6CZAcI2XjN7zcxGEhw9LCRo8oDg6KEqFS2ze3rYXnkdwTeVFmbWnODro8p/ao1ZQfDVt1SnSpb9CnjLzJrH3Jqa2WXlPV9SU4ImqOVlV2RmH5rZKQRfnV8EnglnLSf4gy5dRxOCr/3Lwqyx61eZvF8B3y+Tr5GZvVvJPu0RLeZ+2Syl21u2F+urrnkEzWml+lPBCeFqeh1oLWkAQaF/snRGJe9Nhar6/JrZYjMbE67zN8Cz4Xsbz9/PV8DxZd7XLAvOTZSK/Rzsz9efuW+8h2XmfQUcEMf2k17SFfgKPAD8yMwOBa4B/hxO7w50D094vF/mREh1ZEjKirk1IGh330ZwsqclcGvpwuGJs5PDD+QOgqOr0hNjq4COkjIr2d4qoKr+39kEbahrgAaSbuGbR3CJ8gxwg4KTzB2AKypZ9iWC9+F8SRnh7VuKOfEInCDpiPD1uB34wMzKflPJVHCSupmZFRG0q5e+nk8C4yQNCE+G/SpcRz7wMtBb0mnhe3Yl3/z28NdwX3qH22km6czqvSxA8NqcKGmEpAyCg4AdBO24+yw8AZgVPswMP4ul/6H/A/iJpA4KTjJfTfCtt1Q8n6kKhUf6zwK/I/hPeFKYqbL3pjKVfn4lnScpN/wWtDGcXBIuv6uKffkrcIfCE+aSciWdUmaZn4XfwnsTnCOaEE5/Crg5fE5r4BaC5ikImgPHhe9vWvha18Q381qX9AU+PNobCvxT0iyC9rF24ewGBO2EwwiONv4mqfk+bO4VgmJeersNuIfgZMpagpMyr8Ysn0bwB7ac4FvHUQRtchCcSJsHrJS0toLtjQfOUNCz494KlnmNoA35E4KvkdupvLmkpvyCoA34C+ANgj/6HeUtaGabgVHAOQSvxUqCo7GGMYs9SfCf43rgUIImnPKcD+SHzWE/IDhBh5n9l6Dt+zmCI/YDwu1hZmuBMwlODK8j+Ey8E5PvhTDP0+F65wLHx/tClLO/i8JcfyT4XJwEnGRmO6u7zjIWEXz+OhC8/9v4+mjzfuDfwByC/Xg5nFbqNuDRsNniLKrnSeAY4J9lmnbKfW+qUNXn9zhgnoLeS+MJzgFtD5uF7gDeCfdlcDnrHg/8C3hd0maCv8/DyizzFsEJ8P8Cd5nZ6+H0XwIzCDoPzCE4Cf5LADObTvCfwR8Ivm28xTeP9uuM0hMzSUVSHvCSmfWRlAMsMrN25Sz3V4KTa4+Ej/8LXG9mH9Zm3vpA0mUEf3xHVeO5jxCcoL25xoM5V46whnwBZJT5T6peSfojeDMrAL4o/UqtQGkb5IvA8HB6a4Imm2T6RWSdJamdpMPDr6g9CL6pvBB1Ludc/JKuwEt6iqBvbA9JSyVdTPB1/mJJHxM0e5S2s70GrJM0H5gM/NTM1kWROwVlEnz130zQ3DSRr899OOfqgKRsonHOObfvku4I3jnnXM1IqsGsWrdubXl5eVHHcM65OmPmzJlrzSy3vHlJVeDz8vKYMWNG1Qs655wDQNKSiuZ5E41zzqUoL/DOOZeivMA751yKSqo2eOdc3VJUVMTSpUvZvn171FFSXlZWFh07diQjIyPu53iBd85V29KlS8nOziYvL4+vx0NzNc3MWLduHUuXLqVLly5xP8+baJxz1bZ9+3ZatWrlxT3BJNGqVau9/qbkBd45t0+8uNeO6rzOdb7AFxft5P1HbmT2W89HHcU555JKnS/w6ekNOCj/EbbNnhh1FOdcBJo2rd4V8+655x62bt1a9YIJUN3Me6vOF3ilpbGqQQeaFOZHHcU5V4dEWeBrS50v8ACbm3Sm1Y6lUcdwzkWosLCQESNGcMghh9C3b18mTgy+1W/ZsoUTTzyR/v3706dPHyZMmMC9997L8uXLGT58OMOHD99jXTNnzuSoo47i0EMP5dhjj2XFihUADBs2jKuuuoqhQ4fSp08fpk+fDsD69es59dRT6devH4MHD2b27Nm7M40bN46+ffvSr18/nnvuud3buOmmm+jfvz+DBw9m1apVe2SoCSnRTbK4eVfabnyDrVs207hJdtRxnKuXfv7vecxfXlCj6+zVPodbT+od17JZWVm88MIL5OTksHbtWgYPHszJJ5/Mq6++Svv27Xn55ZcB2LRpE82aNePuu+9m8uTJtG7d+hvrKSoq4kc/+hETJ04kNzeXCRMmcNNNN/Hwww8DwX8Y7777LlOnTuWiiy5i7ty53HrrrRx88MG8+OKLvPnmm1xwwQXMmjWL22+/nWbNmjFnzhwANmzYsHsdgwcP5o477uDaa6/lwQcf5Oaba/6CZylR4DPbdCNtibH8iwUc2GdQ1HGccxEwM2688UamTp1KWloay5YtY9WqVfTt25drrrmG6667jtGjR/Ptb3+70vUsWrSIuXPnMnLkSABKSkpo1+7rK4aOGTMGgCOPPJKCggI2btzItGnTdh+dH3300axbt45Nmzbxxhtv8PTTT+9+bosWLQDIzMxk9OjRABx66KFMmjSp5l6IGClR4Jt1PAg+hI1fLQAv8M5FIt4j7UR54oknWLNmDTNnziQjI4O8vDy2b99O9+7dmTlzJq+88go33HADo0aN4pZbbqlwPWZG7969ee+998qdX7a7oiTKu3BS6fTyujdmZGTsnp6enk5xcWIuG5sSbfD7de0FwI5Vn0ScxDkXlU2bNtGmTRsyMjKYPHkyS5YEo+guX76cxo0bc95553HNNdfw0UcfAZCdnc3mzZv3WE+PHj1Ys2bN7gJfVFTEvHnzds+fMGECANOmTaNZs2Y0a9aMI488kieeeAKAKVOm0Lp1a3Jychg1ahT33Xff7ueWNtHUlpQ4gm+c3ZL1NCN9wxdRR3HOReS73/0uJ510EgMHDmTAgAH07NkTgDlz5vDTn/6UtLQ0MjIy+Mtf/gLApZdeyvHHH0+7du2YPHny7vVkZmby7LPPcuWVV7Jp0yaKi4u56qqr6N07+IbSokULhg4dSkFBwe52+dtuu41x48bRr18/GjduzKOPPgrAzTffzOWXX06fPn1IT0/n1ltv5bTTTqu11ySprsk6cOBAq+4FPxb+aiglBr1vereGUznnKrJgwQIOOuigqGPUmmHDhnHXXXcxcODASLZf3ustaaaZlRsoJZpoALY07UybomXltoU551x9lBJNNAC7Wh5I7vpXWL9xAy1btIw6jnMuBU2ZMiXqCHslZY7gG7XtBsDKz+dGnMQ555JDyhT4Fp2CdqlNyxZGnMQ555JDyhT4tnlBgS9evTjiJM45lxxSpsA3yGrKarUiY6N3lXTOOUihAg+wruH+NNv2ZdQxnHO16KKLLqJNmzb06dPnG9PXr1/PyJEj6datGyNHjtz9I6NZs2bxyiuv7F7utttu46677qrVzGXl5+fvkb8mpFSB35adx37Fy9i1y7tKOldfXHjhhbz66qt7TL/zzjsZMWIEixcvZsSIEdx5553AngU+laVUgafVAbRQIStWLIs6iXOulhx55JG0bLln1+iJEycyduxYAMaOHcuLL77Izp07ueWWW5gwYQIDBgzYPezA/PnzGTZsGF27duXee+8tdzuvv/46Q4YM4ZBDDuHMM8+ksLAQgLy8PK677joGDRrEoEGD+PTTTwFYsmQJI0aMoF+/fowYMYIvvwxaF1atWsV3vvMd+vfvT//+/Xn33eDHmSUlJVxyySX07t2bUaNGsW3btn1+bVKmHzxA43Y9YCGsXjKfDh06Rh3HufrlP9fDyjk1u879+sLxd1brqatWrdo9CmS7du1YvXo1mZmZ/OIXv2DGjBm7x4i57bbbWLhwIZMnT2bz5s306NGDyy67jIyMjN3rWrt2Lb/85S954403aNKkCb/5zW+4++67dw9alpOTw/Tp0/nHP/7BVVddxUsvvcQVV1zBBRdcwNixY3n44Ye58sorefHFF7nyyis56qijeOGFFygpKaGwsJANGzawePFinnrqKR588EHOOussnnvuOc4777x9evlS6gg+t3PQk6Zw+aKIkzjn6pITTzyRhg0b0rp1a9q0abPHBTjef/995s+fz+GHH86AAQN49NFHdw9mBl8PITxmzJjdg5S99957nHvuuQCcf/75TJs2DYA333yTyy67DAhGkmzWrBkAXbp0YcCAAUAwhHB+fv4+71dKHcG37NidEhO71n4adRTn6p9qHmknStu2bVmxYgXt2rVjxYoVtGnTpsJlGzZsuPt+ecP3mhkjR47kqaeeKvf5sUMClzc8cGXTK8pQE000KXUErwYNWZ3eloYF+VFHcc5F7OSTT949quOjjz7KKaecAlQ8THBlBg8ezDvvvLO7fX3r1q188snXw5OXtuVPmDCBIUOGADB06NDdF/t44oknOOKIIwAYMWLE7hEtS0pKKCio2atgxUqpAg+wodH+tPSuks7VG2PGjGHIkCEsWrSIjh078tBDDwFw/fXXM2nSJLp168akSZO4/vrrARg+fDjz58//xknWquTm5vLII48wZsyY3dddXbjw61/N79ixg8MOO4zx48fzhz/8AYB7772Xv//97/Tr14/HHnuM8ePHAzB+/HgmT55M3759OfTQQ78x1nxNS5nhgkv97/5L6bZ8Ig1uWkZWZkq1QDmXdOrbcMHlycvLY8aMGXtc2zURkm64YEnpkv4n6aVEbwsgvfUBNNV2li1bUvXCzjmXwmqjiebHwIJa2A4ATTsEV3FZmz+/tjbpnKvH8vPza+XovToSWuAldQROBP6WyO3EapMXXJ9160q/PqtztSGZmnlTWXVe50Qfwd8DXAvsqmgBSZdKmiFpxpo1a/Z5g03bdKWIBrDOR5V0LtGysrJYt26dF/kEMzPWrVtHVlbWXj0vYWchJY0GVpvZTEnDKlrOzB4AHoDgJOs+bzgtnVXp7Wi82dvgnUu0jh07snTpUmri4MxVLisri44d9+4X+onsZnI4cLKkE4AsIEfS42a2b7+9jUNB48609gLvXMJlZGTQpUuXqGO4CiSsicbMbjCzjmaWB5wDvFkbxR2gqHkXOthKNm3dURubc865pJRyP3QCyMg9kCwVsXSJD1ngnKu/qizwkvb4/lXetMqY2RQzG703z9kXzcLrs2740rtKOufqr3iO4J8rZ9qzNR2kJuV2DrpKbl/lXSWdc/VXhSdZJfUEegPNJJ0WMyuH4KRp0sps3oFtNCRt/edRR3HOuchU1oumBzAaaA6cFDN9M3BJAjPtu7Q01mR0oOkW70njnKu/KizwZjYRmChpiJm9V4uZakRhk/1ps2ERZlblOMzOOZeKqmyDr4vFHaCkxQF0YDWrNm6JOopzzkUiJbtJAmTt150MlbA8f2HVCzvnXApK2QLfIuwquWmpF3jnXP0UTz/4VpL+KOkjSTMljZfUqjbC7YuWYYHfudoHHXPO1U/xHME/DawGTgfOANYA8V3nKkJpTXPZQmMabPgs6ijOOReJeAp8SzO73cy+CG+/JOg6mdwkNjTan6abP6NwR3HVyzvnXIqJp8BPlnSOpLTwdhbwcqKD1YT0A4ZxCIt463+1dkEp55xLGvEU+O8DTwI7gR0ETTY/kbRZUkEiw+2rtkPPJUMlrJn+z6ijOOdcrYunH3y2maWZWQMzywjvZ4e3nNoIWV1p7fqxLiuPnmsnsa7Qhw52ztUv8fSikaTzJP0sfNxJ0qDER6sBErv6nMYgLWDyhx9HncY552pVPE00fwaGAOeGjwuBPyUsUQ3LPWwMaTIKPkrqATCdc67GxVPgDzOzy4HtAGa2AchMaKqalNudNU17MGDTf/lq/dao0zjnXK2Jp8AXSUoHDEBSLrAroalqWEa/Mzgk7VOmfPBh1FGcc67WxFPg7wVeANpIugOYBvw6oalqWPNvnQ1A0cfeTOOcqz8qGw8eADN7QtJMYAQg4FQzq1sdy1t0ZnXz/gxe/xYLVxbQc7+k7vzjnHM1Ip5eNI+Z2UIz+5OZ3WdmCyQ9VhvhalLjQ86iV9oSpr33TtRRnHOuVsTTRNM79kHYHn9oYuIkTtODz2AXQnNfwMyijuOccwlXYYGXdIOkzUA/SQXhbTPBwGMTay1hTcnej7Wtv8Wwoql8tGR91Gmccy7hKizwZvZrM8sGfmdmOeEt28xamdkNtZixxuQMPIcD0lbwwXtvRR3FOecSLp4mmpckNQEIf9F6t6TOCc6VEFn9vkMJ6TT+ZCJFJXWqp6dzzu21eAr8X4CtkvoD1wJLgH8kNFWiNG7J+v2O4Jhd03hn8Zqo0zjnXELFU+CLLTgreQow3szGA9mJjZU4zQedQ0et5eP3JkUdxTnnEiqeAr9Z0g3AecDLYS+ajMTGSpyMXqMpUiYt819iZ7E30zjnUlc8Bf5sgnHgLzazlUAH4HcJTZVIWTmsbz+MY3mPmfnrok7jnHMJE8948CvN7G4zezt8/KWZ1c02+FDOgFNoo40s/Pi9qKM451zCxHMEn3Ia9TgaAPtscsRJnHMuceplgSenPesad+XAwg/9Sk/OuZQVz1g0P45nWl2zq8swBmkh7y5aFnUU55xLiHiO4MeWM+3CGs5R61r2PZYsFbFs9pSoozjnXEJUOFywpDEEl+nrIulfMbOygTrf/SS9yxGUkE6jr97CbBySoo7knHM1qrLx4N8FVgCtgd/HTN8MzK5qxZKygKlAw3A7z5rZrdWPWsMaNmVdi/4csm4Wi1cX0r1tnf3tlnPOlavCAm9mSwiGJRhSzXXvAI42s0JJGcA0Sf8xs/erub4al9XzGHq/+zuemPsJ3dvWuRGQnXOuUvGcZB0s6UNJhZJ2SiqRVFDV8yxQGD7MCG9JNRB7Tq9RpMnYNP/NqKM451yNi+ck633AGGAx0Aj4HvDHeFYuKV3SLIIx5CeZ2QflLHOppBmSZqxZU8sDgLU/mO3pTWmz5l22F5XU7radcy7B4uoHb2afAulmVmJmfweGx/m8EjMbAHQEBknqU84yD5jZQDMbmJubuxfRa0B6AwraDWGoZjPjC78IiHMutcRT4LdKygRmSfqtpP8DmuzNRsxsIzAFOG6vEyZY894j6ai1zJn7UdRRnHOuRsVT4M8Pl7sC2AJ0Ak6v6kmSciU1D+83Ao4BFlY7aYJkdj8GgJLF3g7vnEstlXWTBHb3pgHYDvx8L9bdDng0HF44DXjGzF7a+4gJ1rIrBQ3bcWDhTFYXbKdNTlbUiZxzrkYkbCwaM5ttZgebWT8z62Nmv0jUtvaJRHHeUQxNm8c7n6yMOo1zztWY+jnYWBnN+xxLjraSP2da1FGcc67GxNMP/sx4ptVlaV2PYhci66up7NqVVF31nXOu2uI5gr8hzml1V5NWbGp2EIcUf8yClVX+hss55+qEygYbOx44Aegg6d6YWTlAcaKD1bbM7kdzyPS/8I8FS+jdvl/UcZxzbp9VdgS/HJhB0HtmZsztX8CxiY9Wu5ocNJIMlbB+nneXdM6lhsoGG/sY+FjSk+Fy+5vZolpLVts6DaZImeSueZ+tO4tpnFllD1LnnEtq8bTBHwfMAl4FkDSgzPjwqSEji8L9BjFUs3nn0zo/3L1zzsVV4G8DBgEbAcxsFpCXqEBRyu41ku5py3h/1pyoozjn3D6Lp8AXm9mmhCdJAg26jwJAi1+juGRXxGmcc27fxFPg50o6F0iX1E3SHwmu9pR62hzEliadOLz4A2Ys2RB1Guec2yfxFPgfAb0JrtD0FFAAXJXATNGRyOg1msPT5jF59hdRp3HOuX1SZYE3s61mdpOZfSsct/0mM9teG+GikNnrRDJVTOG81zDzX7U65+quKvsCSvo3e15qbxNBH/n7U67Y7z+EHRnNOGT7e8xfUUDv9s2iTuScc9USTxPN50Ah8GB4KwBWAd3Dx6klvQHWbRRHp/2PSXOXRZ3GOeeqLZ5f8xxsZkfGPP63pKlmdqSkeYkKFqWs3qPJmv9Pls+eAqN6RR3HOeeqJZ4j+FxJ+5c+CO+3Dh/uTEiqqB04ghJl0G3j23y1fmvUaZxzrlriKfBXA9MkTZY0BXgb+KmkJsCjiQwXmYbZ7Nz/24xMm8lrc1dEncY556olnkv2vSKpG9ATELAw5sTqPQnMFqlGfUaTt+RN5s3+EI48IOo4zjm31yobLvi0CmZ1lYSZPZ+gTMmhxwnw8k9ov/JN1hV+h1ZNG0adyDnn9kplR/Anhf+2AYYC/yU4gh8OTAFSu8DntGNrbn+OWTWT/y5YzVnf6hR1Iuec2ysVtsGb2TgzG0fQB76XmZ1hZqcT/Kq1XmjUZzQHp33K+7PnRx3FOef2WjwnWfPMLPZMY2kf+JSnnicC0Dh/Elt2pNxFrJxzKS6eAj9F0muSLpQ0FngZmJzgXMmhTS+2N+nIMGYw9ZM1Uadxzrm9Es9YNFcA9wP9gQHAA2b2owTnSg4Smb1Hc0T6XKbMzY86jXPO7ZW4rksX9phJ7ZOqFUg76ESypv+VHYveoKjkMDLS4/nS45xz0auyWknaLKkgvG2XVCKpoDbCJYX9h1CUkcPhxdP54PP1Uadxzrm4xdNEk21mOeEtCzgduC/x0ZJEegZ0H8XR6f9j3jIv8M65umOv2xvM7EXg6JqPkrwyeo2mlTbDV9OjjuKcc3GLZzz42F+0pgED2XN8+NTW5SgAmq79H3BetFmccy5O8ZxkPSnmfjGQD5ySkDTJqnFLCtNzaFz4VdRJnHMubvEMNjauNoIku4JG+5NbsIyikl3ek8Y5VyfE04umq6R/S1ojabWkiZK61ka4ZFLUrDOdWcWyDduijuKcc3GJ51D0SeAZoB3QHvgn8FQiQyWjBq0PpL3W8uWaDVFHcc65uMRT4GVmj5lZcXh7nDhOskrqFF4kZIGkeZJ+vO9xo9Nkv26ky9iw7NOoozjnXFwqLPCSWkpqCUyWdL2kPEmdJV1LMB5NVYqBq83sIGAwcLmkOnuB05z2wfhq21ctjjiJc87Fp7KTrDMJjtQVPv5+zDwDbq9sxeEIlCvC+5slLQA6AHVy7N201sFVnWz95xEncc65+FRY4M2sS01tRFIecDDwQU2ts9Y1bsVWNabR5i+jTuKcc3FJeH8/SU2B54CrzGyPMWwkXSpphqQZa9Yk8ZC8EhsbdaLljqWY1a/feTnn6qaEFnhJGQTF/YmKruFqZg+Y2UAzG5ibm5vIOPtsR3ZnOtoK1hbujDqKc85VKWEFXpKAh4AFZnZ3orZTm9TqADpqLV+u3Rh1FOecq1I8Y9EcUs7kTcASM6vsOnaHA+cDcyTNCqfdaGav7HXKJNF4v25kzC9h7dLPoEvbqOM451yl4hmL5s/AIcBsgh41fcL7rST9wMxeL+9JZjaNr3vgpITmHXsAsHXlYmBotGGcc64K8TTR5AMHh+3khxL0hpkLHAP8NoHZkk5m7oEA7FrrXSWdc8kvngLf08zmlT4ws/kEBb/+VbmmbdmuhmRuzo86iXPOVSmeJppFkv4CPB0+Phv4RFJDoChhyZKRxPqGHWm+zYcNds4lv3iO4C8EPgWuAv4P+DycVgQMT1CupLWtaWfalaxg687Kzi8751z04hkPfhvw+/BWVmGNJ0py1qILndZM5Yu1m+nZvkXUcZxzrkLxjAd/uKRJkj6R9HnprTbCJaOstt1oqGJWLfsi6ijOOVepeNrgHyJompkJlCQ2TvJr3iHoKlm4/BOC3qPOOZec4inwm8zsPwlPUkc0DYcNLl7zWcRJnHOucvEU+MmSfgc8D+wonWhmHyUsVTLLbs9OMmiwKT/qJM45V6l4Cvxh4b8DY6YZcHTNx6kD0tJYl9me7K0+bLBzLrnF04um3nWFrEphk/1psy6fkl1GelpKjcbgnEshFRZ4SeeZ2eOSflLe/FQZIbI6Sprl0Xn9ByzfsJVOrZpEHcc558pVWTfJ0sqVXcGt3spscyCNtJOVy/KjjuKccxWq7JJ994f//rz24tQN2WFPmk3LF0G/3hGncc658sUzHnwucAmQF7u8mV2UuFjJrWWngwDYuerTiJM451zF4ulFMxF4G3gD/6ETAOnNO1FMOmkb/deszrnkFU+Bb2xm1yU8SV2S3oC1DfajyRbvKumcS17xjCb5kqQTEp6kjilo3IlWO5ZhZlFHcc65csVT4H9MUOS3SSqQtFlSQaKDJbuinDw6sZKNW3ZGHcU558pVZYE3s2wzSzOzRmaWEz7OqY1wySy99YFkaxvLlvvFP5xzyamyHzr1NLOFksodMrHejkUTym7fDWbB+q8WQvcDo47jnHN7qOwk60+ASyn/Qh/1dyyaUKuwq+SO1YsjTuKcc+Wr7IdOl4b/+lg05cjK7UIJabDeu0o655JTPN0kkdQH6AVklU4zs38kKlSd0CCTteltyNrsXSWdc8kpnl+y3goMIyjwrwDHA9OA+l3ggY1ZHWm5dWnUMZxzrlzxdJM8AxgBrDSzcUB/oGFCU9URO7LzaL9rBduL/Ae+zrnkE0+B32Zmu4BiSTnAaqBrYmPVDWrVlRYqZPmKZVFHcc65PcRT4GdIag48SHDh7Y+A6YkMVVc03i/oHrn2y0URJ3HOuT3Fc0WnH4Z3/yrpVSDHzGYnNlbd0LJTTwC2rlwMjIw2jHPOlRFvL5p+xAwXLOlAM3s+gbnqhObtuwFQuOKTiJM459ye4ulF8zDQD5gH7AonG1DvC7wyG7OmURcOXTuRFyafzXeGD4k6knPO7RbPEfxgM+uV8CR1VMsLHmP7g8fRf8o4pudOZFCfHlFHcs45IL6TrO9J8gJfgfR2fdG5E2ivdTR59hy+WLYi6kjOOQfEV+AfJSjyiyTNljRHkp9kjdH4wCMoGP0QPchn48Nnsqlgc9SRnHMurgL/MHA+cBxwEjA6/LdSkh6WtFrS3H2LWDe0GXgyS464i4NL5rDoL+dQVFQUdSTnXD0XT4H/0sz+ZWZfmNmS0lscz3uE4D+FeuOAYy7mf72uY9C2aXz813HgV3tyzkUongK/UNKTksZIOq30VtWTzGwqsH7fI9YtB591I++0v5CB6/7N7Mf9UrbOuejEU+AbATuAUQRNM6XNNDVC0qWSZkiasWbNmppabaQGX/wH3m08nJ6fPcTW9T4YmXMuGvH8knVcIgOY2QPAAwADBw5MiTaN9PQ0so+7lfTnhrP4pd/T+4I/RB3JOVcPVXkEL6m7pP+WniyV1E/SzYmPVrf16TuAaZlHkPf509j2TVHHcc7VQ/E00TwI3AAUAYTj0JyTyFCpQBLbBl1OE7ay9I2/RB3HOVcPxVPgG5tZ2dEji6t6kqSngPeAHpKWSrq4OgHrsiOPGsn79CV71oNQvDPqOM65eiaeAr9W0gEE488g6Qygyp9rmtkYM2tnZhlm1tHMHtrHrHVO48wGfNbtYpoXr6Vg+hNRx3HO1TPxFPjLgfuBnpKWAVcBP0hkqFQydNSZzN/VmaK3x8OuXVU/wTnnakiVBd7MPjezY4BcoCfB9VmPSHCulNEltylv5Z5Lq21fULzoP1HHcc7VIxUWeEk5km6QdJ+kkcBWYCzwKXBWbQVMBd2Hn89Sa03BG7+POopzrh6p7Aj+MaAHMAe4BHgdOBM41cxOqYVsKWNYr/b8M+NUWq6bCV9+EHUc51w9UVmB72pmF5rZ/cAYYCAw2sxm1UqyFJKeJrKHjmODNWXzm34U75yrHZUV+N3DIZpZCfCFmfk4uNV02mHdecKOJTv/NVjjl/hzziVeZQW+v6SC8LYZ6Fd6X1JBbQVMFS2bZLK651i2WSY7374n6jjOuXqgwgJvZulmlhPess2sQcz9nNoMmSpO/3Z/JpQMI3P2E/DMWFi9MOpIzrkUFk8/eFdD+ndqzittv8+jGWeya/Ek+PNgeO57sHZx1NGccynIC3wt++Gofty5/Qy+Vfh73mw9hl0LXoI/DYIXLoP1n0cdzzmXQrzA17JhPdow9drhnHp4fy5beTKDt/6Bt1udic19Hu77Fix4KeqIzrkUIUuiy8oNHDjQZsyYEXWMWrOqYDt/nvwpT03/itas55lm99Fh5+fo/Beh85Co4znn6gBJM81sYHnz/Ag+Qm1zsvj5KX2Y8tNhDB/Yj5M3XMkqtcaeOhtWL4g6nnOujvMCnwTaN2/EHd/py49PGsIZhT9lc3ED7PHTYZNf7s85V31e4JPI2KF5nPDtwzh7yzXs3LIJHj8dtta765Y752qIF/gkc/1xPenadzBjt11FybrP4KkxULQt6ljOuTrIC3ySSUsTvz+zP7v2P4Krdl6OffUBPHsxlFR5ES3nnPsGL/BJKCsjnQcuOJT5LYbza8bBopfh2Qv9SN45t1e8wCep5o0zeWTcIJ5vcCLjG1yELXgJHj0JCtdEHc05V0d4gU9inVo25uELB3L/zmO5scFP2bViNjx0jA9t4JyLixf4JNevY3Oe+f4QJqcN5tyin7FzawH87RjIfyfqaM65JOcFvg7o06EZE684nK1tDuaYgp+xMa059tipMPufUUdzziUxL/B1RNucLCZcOoS+fQZw1Pob+aLhQfD892Dyr2FXSdTxnHNJyAt8HdIoM50/jjmYsUcP4Lj1P+GtRsfAW3fCY9+BwtVRx3POJRkv8HVMWpr4yage/Pbsb3HJ5ov5XcMr2PXlB/DXI+CLqVHHc84lES/wddSpB3fgye8N5omioxjDHWxPbwr/OAWm/MabbJxzgBf4Om1gXkue/cFQlmV25YgNt7Cy80kw5VfeZOOcA7zA13kHtmnK8z8cStvWrTj8k7OZ3u8X8NUHQVfK7Zuijueci5AX+BTQJjuLCd8fwuEH5nLW9AOZcNB92Kal8NJPIIku6OKcq11e4FNE04YNeGjsQM44tCPXfdiYSW3GwdxnYfaEqKM55yLSIOoAruZkpKfxuzP6kZvdkB9M2cWU3Bns//LV0GkQtOwadTznXC3zI/gUI4lrj+3BuYPzOGfNRezYJXjue1BSFHU051wt8wKfgiTx85P70L9PH67aehEsmwlT7ow6lnOulnmBT1HpaeIPZw9gQ97xPFMyHHv795A/LepYzrlalNACL+k4SYskfSrp+kRuy+0puHDIQJ5q9UOW2H7s/Of3/BqvztUjCSvwktKBPwHHA72AMZJ6JWp7rnw5WRncf9GR3J51Ndqyhq1PXwTzXoAl78H6z2Hn1qgjOucSJJG9aAYBn5rZ5wCSngZOAeYncJuuHG1ysrj5knMZ/6d5XPPl3+HLyd+Yv4XGbEhrTjHpESV0rn7bmt6MXjfV/DUeElngOwBfxTxeChxWdiFJlwKXAuy///4JjFO/dWndhOMv+Tk3TD6VpjtWk1O8nmYl62hWvJ6c4nVkl2wkDR/DxrkoFGfkJGS9iSzwKmfaHj+rNLMHgAcABg4c6D+7TKDe7Zvx6+8OizqGc66WJPIk61KgU8zjjsDyBG7POedcjEQW+A+BbpK6SMoEzgH+lcDtOeeci5GwJhozK5Z0BfAakA48bGbzErU955xz35TQsWjM7BXglURuwznnXPn8l6zOOZeivMA751yK8gLvnHMpygu8c86lKFkSXdJN0hpgSTWf3hpYW4Nx6grf7/rF97t+iWe/O5tZbnkzkqrA7wtJM8xsYNQ5apvvd/3i+12/7Ot+exONc86lKC/wzjmXolKpwD8QdYCI+H7XL77f9cs+7XfKtME755z7plQ6gnfOORfDC7xzzqWoOl/g69OFvSU9LGm1pLkx01pKmiRpcfhviygz1jRJnSRNlrRA0jxJPw6np/p+Z0maLunjcL9/Hk5P6f0uJSld0v8kvRQ+ri/7nS9pjqRZkmaE06q973W6wNfDC3s/AhxXZtr1wH/NrBvw3/BxKikGrjazg4DBwOXhe5zq+70DONrM+gMDgOMkDSb197vUj4EFMY/ry34DDDezATH936u973W6wBNzYW8z2wmUXtg7JZnZVGB9mcmnAI+G9x8FTq3NTIlmZivM7KPw/maCP/oOpP5+m5kVhg8zwpuR4vsNIKkjcCLwt5jJKb/flaj2vtf1Al/ehb07RJQlKm3NbAUExRBoE3GehJGUBxwMfEA92O+wmWIWsBqYZGb1Yr+Be4BrgV0x0+rDfkPwn/jrkmZKujScVu19T+gFP2pBXBf2dnWfpKbAc8BVZlYglffWpxYzKwEGSGoOvCCpT8SREk7SaGC1mc2UNCziOFE43MyWS2oDTJK0cF9WVteP4P3C3rBKUjuA8N/VEeepcZIyCIr7E2b2fDg55fe7lJltBKYQnH9J9f0+HDhZUj5Bk+vRkh4n9fcbADNbHv67GniBoBm62vte1wu8X9g72N+x4f2xwMQIs9Q4BYfqDwELzOzumFmpvt+54ZE7khoBxwALSfH9NrMbzKyjmeUR/D2/aWbnkeL7DSCpiaTs0vvAKGAu+7Dvdf6XrJJOIGizK72w9x3RJkocSU8BwwiGEF0F3Aq8CDwD7A98CZxpZmVPxNZZko4A3gbm8HWb7I0E7fCpvN/9CE6opRMciD1jZr+Q1IoU3u9YYRPNNWY2uj7st6SuBEftEDSfP2lmd+zLvtf5Au+cc658db2JxjnnXAW8wDvnXIryAu+ccynKC7xzzqUoL/DOOZeivMC7OklSq3DEvVmSVkpaFt4vlPTnWsowIOymW1Pru7Gm1uUceDdJlwIk3QYUmtldtbzdC4GBZnZFDa2v0Mya1sS6nAM/gncpRtKwmDHEb5P0qKTXw3G2T5P023C87VfDIRCQdKikt8IBnl4r/Vl4mfWeKWluOD771PCX078Azg6/OZwd/hLxYUkfhmOZnxI+90JJE8NtLpJ0aznrvxNoFK7riYS+SK7e8ALvUt0BBEPPngI8Dkw2s77ANuDEsMj/ETjDzA4FHgbK+zX0LcCx4fjsJ4fDU98CTAjH7p4A3ETw0/pvAcOB34U/OYdgTJHvEoztfqakgbErN7PrgW3hur5bg/vv6rG6Ppqkc1X5j5kVSZpD8LP/V8Ppc4A8oAfQh2DkPsJlVpSznneARyQ9AzxfznwIxg45WdI14eMsgp+XQzDc7zoASc8DRwAz9mG/nKuSF3iX6nYAmNkuSUX29UmnXQSffwHzzGxIZSsxsx9IOozg28AsSQPKWUzA6Wa26BsTg+eVPdnlJ79cwnkTjavvFgG5koZAMDSxpN5lF5J0gJl9YGa3AGsJhqneDGTHLPYa8KNwBEwkHRwzb6SCa2s2IrgizzvlZCkqPS/gXE3wAu/qtbAt/QzgN5I+BmYBQ8tZ9Hfhydm5wFTgY2Ay0Kv0JCtwO8Gl9WaHy90e8/xpwGPh+p8zs/KaZx4In+snWV2N8G6SziVYTXendC5efgTvnHMpyo/gnXMuRfkRvHPOpSgv8M45l6K8wDvnXIryAu+ccynKC7xzzqWo/wf5yp7L4fNVDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the budget spending at the last episode of training\n",
    "ax = last_train_episode['rem_budget'].head(50).plot()\n",
    "ax = last_train_episode_10['rem_budget'].head(50).plot()\n",
    "ax.set_xlabel(\"Time step t\")\n",
    "ax.set_ylabel(\"Remaining budget at step t\")\n",
    "ax.set_title(\"Last training episode from 10th vs last epoch\")\n",
    "ax.legend(['last epoch', '10th epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above it seems obvious that the starting $\\lambda_0$ is too high, since there is a very sharp drop in the remaining budget in the first 10 steps (out of 96) in both subsets of the training history. Basically because of that the model is not able to learn properly, i.e. learn to spend the budget under given constraints. We strongly believe that trying out different solutions for setting $\\lambda_0$ should be considered as a potential improvement of the DRLB. \n",
    "\n",
    "The graph below shows that the DQN was making sensible steps in adjusting the $\\lambda$ parameter by increasing it for the first 10 steps, which reinforces our suspicions about the $\\lambda_0$. Once the agent exhausted its budget though, the DQN actions obviously cannot be evaluated properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb244b46b20>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the lambda parameters in the last training episode\n",
    "ax = last_train_episode['ctl_lambda'].head(50).plot()\n",
    "ax = last_train_episode_10['ctl_lambda'].head(50).plot()\n",
    "ax.set_xlabel(\"Time step t\")\n",
    "ax.set_ylabel(\"Lambda parameter at time step t\")\n",
    "ax.set_title(\"Last training episode from 10th vs last epoch\")\n",
    "ax.legend(['last epoch', '10th epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. References \n",
    "\n",
    "Altman E. (1999): „Constrained Markov decision processes“, CRC Press, Vol. 7.  \n",
    "\n",
    "Dantzig G. 1957. Discrete-variable extremum problems. Operations research 5, 2 (1957), 266–288.\n",
    "\n",
    "Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and Defeng Guo. 2017. Real-Time Bidding by Reinforcement Learning in Display Advertising. In Proceedings of the Tenth ACM International Conference on Web\n",
    "Search and Data Mining. ACM, 661–670.\n",
    "\n",
    "Singh A. (2019): „Reinforcement Learning : Markov-Decision Process (Part 1)“, [online] https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da [08.12.2020].  \n",
    "\n",
    "Stekolshchik R. (2020): „A pair of interrelated neural networks in Deep Q-Network“, [online] https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4 [09.12.2020].\n",
    "\n",
    "Richard S. and Andrew G. (2018): \"Reinforcement Learning\", The MIT Press,pp.1-6. \n",
    "\n",
    "Mnih V., Kavukcuoglu K., Silver D., Rusu A.A., Veness J.  Bellemare M., Graves A., Riedmiller M.A., Fidjeland A., Ostrovski G., Petersen S., Beattie C., Sadik A., Antonoglou I., King H., Kumaran D., Wierstra D., Legg S., Hassabis D. (2015): \"Human-level control through deep reinforcement learning\", Nature 518, 7540, pp. 529–533.\n",
    "\n",
    "Reza R.A, Yingqian Z., Murat F. et al., A State Aggregation Approach for Solving Knapsack Problem with Deep Reinforcement Learning,Proceedings of Machine Learning Research 129:81–96, 2020\n",
    "\n",
    "Wu D., Chen X., Yang X., et al. (2018): „Budget Constrained Bidding by model-free Reinforcement Learning in Display Advertising“, Association for Computing Machinery, 27, pp. 1443–1451.  \n",
    "\n",
    "Zhang W., Yuan S., and Wang J. (2014): „Optimal Real-Time Bidding for Display Advertising“ Association for Computing Machinery, 20, pp. 1077–1086. \n",
    "\n",
    "Zhang W., Yuan S., Wang J. (2014): \"Real-Time Bidding Benchmarking with iPinYou Dataset\", arXiv:1407.7073 [cs.GT]. \n",
    "\n",
    "Zhang W., Ren K., and Wang J (2016): \"Optimal real-time bidding frameworks discussion\", arXiv:1602.01007. \n",
    "\n",
    "Zhou Y., Chakrabarty D., and Lukose R. (2008): „Budget constrained bidding in keyword auctions and online knapsack problems“, International Workshop on Internet and Network Economics, Springer, pp. 566–576.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
